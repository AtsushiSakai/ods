\chapter{配列を使ったリスト}
\chaplabel{arrays}

この章では、\emph{backing array}と呼ばれる配列にデータを格納する、#List#・#Queue#インターフェースの実装について検討する。
\emph{backing array}.
\index{backing array}%
次の表は、この章で説明するデータ構造の操作時間を要約したものだ。

\newlength{\tabsep}
\setlength{\tabsep}{\itemsep}
\addtolength{\tabsep}{\parsep}
\addtolength{\tabsep}{-2pt}
\begin{center}
\vspace{\tabsep}
\begin{tabular}{|l|l|l|} \hline
 & #get(i)#/#set(i,x)# & #add(i,x)#/#remove(i)# \\ \hline
#ArrayStack# & $O(1)$ & $O(#n#-#i#)$ \\
#ArrayDeque# & $O(1)$ & $O(\min\{#i#,#n#-#i#\})$ \\
#DualArrayDeque# & $O(1)$ & $O(\min\{#i#,#n#-#i#\})$ \\
#RootishArrayStack# & $O(1)$ & $O(#n#-#i#)$ \\ \hline
\end{tabular}
\vspace{\tabsep}
\end{center}

データをひとつの配列に入れるデータ構造には以下のような利点・欠点がある。

\begin{itemize}
  \item 配列の任意の要素には一定の時間でアクセスできる。
  そのため#get(i)#・#set(i,x)#はいずれも定数時間で実行される。

  \item 配列は動的ではない。リストの真ん中付近に要素を追加・削除するためには、隙間を作ったり埋めたりするために多くの要素が移動することになる。
  #add(i,x)#・#remove(i)#操作の実行時間が#n#・#i#に依存するのはこのためだ。

  \item 配列は伸び縮みしない。
  backing arrayのサイズより多くの要素をデータ構造に入れるには、新しい配列を割当て、古い配列の要素をそちらにコピーする必要がある。
  この操作のコストは大きい。
\end{itemize}

3つめの点が重要だ。
上記の表に記載された実行時間はbacking arrayの拡大・縮小にかかるコストは含まれていない。
後述するように、注意深く設計すれば、backing arrayの拡大・縮小のコストを加味しても\emph{平均的な}実行時間はほとんど増えない。
より正確に言うと、空のデータ構造からはじめて、#add(i,x)#・#remove(i)#を#m#回実行するときの、backing arrayの拡大・縮小のための合計コストは$O(m)$である。
個々の操作のコストは大きいが、#m#個の操作にわたる償却コストを考えれば、ひとつの操作あたりのコストは$O(1)$なのだ。

\section{ArrayStack：配列を使った高速なスタック操作}
\seclabel{arraystack}

\index{ArrayStack@#ArrayStack#}%

#ArrayStack#は\emph{backing array}#a#を使ったリストインターフェースの実装だ。
リストの#i#番目の要素を#a[i]#とする。
ほとんどの場合#a#は実際に必要な値よりも大きい。
そのため整数#n#によって実際に#a#に入っている要素数を表す。
つまり、リストの要素は#a[0]#,\ldots,#a[n-1]#に格納される。
また、関係$#a.length# \ge #n#$が常に成り立つ。

\codeimport{ods/ArrayStack.a.n.size()}

\subsection{基本}
#get(i)#や#set(i,x)#を使って#ArrayStack#の要素を読み書きする方法は簡単である。
境界チェックをしたあと単に#a[i]#を返すか、#a[i]#を書き換えるかすればよいのだ。

\codeimport{ods/ArrayStack.get(i).set(i,x)}

#ArrayStack#に要素を追加・削除するための実装を\figref{arraystack}に示す。
#add(i,x)#では、まず#a#が既に一杯かどうかを調べる。
もしそうなら#resize()#を呼び出して、#a#を大きくする。
#resize()#の実装方法については後述する。
#resize()#の直後では$#a.length# > #n#$であることだけ知っていれば今は十分である。
あとは#x#が入るように$#a[i]#,\ldots,#a[n-1]#$をひとつずつ右に移動させ、#a[i]#をxにし、#n#を1増やせばよい。

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/arraystack}
  \end{center}
  \caption[Adding to an ArrayStack]{
  #add(i,x)#・#remove(i)#を#ArrayStack#に実行する。
  矢印は要素のコピーを表す。
  #resize()#を呼ぶ操作にはアスタリスクを付した。}
  \figlabel{arraystack}
\end{figure}

\codeimport{ods/ArrayStack.add(i,x)}
#resize()#が呼ばれるかもしれないが、このコストを無視すれば#add(i, x)#のコストは#x#を入れる場所を作るためにシフトする要素数に比例する。つまり、この操作のコストは（リサイズのことを無視すれば）$O(#n#-#i#)$である。

#remove(i)#の実装も似ている。
$#a[i+1]#,\ldots,#a[n-1]#$を左にひとつシフトし（#a[i]は書き換えられる））、#n#の値をひとつ小さくする。
その後#n#が#a.length#より小さすぎないか、具体的には$#a.length# \ge 3#n#$を確認する。
もしそうなら、#resize()#を呼んで#a#を小さくする。

\codeimport{ods/ArrayStack.remove(i)}
#resize()#が呼ばれるかもしれないが、このコストを無視すれば#remove(i)#のコストはシフトする要素数に比例し、$O(#n#-#i#)$である。

\subesction{拡張と収縮}

#resize()#の実装は単純だ。
大きさ$2#n#$の新しい配列#b#を割当て、#n#個の#a#の要素を#b#のはじめの#n#個としてコピーする。そして#a#を#b#に置き換える。
よって#resize()#の呼び出し後は$#a.length# = 2#n#$が成り立つ。

\codeimport{ods/ArrayStack.resize()}

#resize()#の実際のコストの計算も簡単だ。
大きさ$2#n#$の配列#b#を割当て、#n#要素をコピーする。
これは$O(#n#)$の時間がかかる。

前節からの実行時間分析は#resize()#のコストを無視していた。
この節では\emph{償却解析}として知られる方法でこれを解決する。
この手法は個々の#add(i, x)#・#remove(i)#における#resize()#のコストを求めるわけではない。
代わりに、#m#個の#add(i, x)#・#remove(i)#からなる操作列の間に呼ばれる#resize()#すべてのことを考える。

次の補題を示す。
\begin{lem}\lemlabel{arraystack-amortized}
  空の#ArrayStack#が作られたあと、$m\ge 1$個の#add(i, x)#・#remove(i)#からなる操作の列が順に実行されるとき、この間に呼ばれる#resize()#の合計実行時間は#O(m)#である。
\end{lem}

%XXX: ここまで完了
\begin{proof}
#resize()#が呼ばれるとき、その前の#resize()#呼び出しから#add#・#remove#が実行された回数は$#n#/2-1$以下である。
$i$回目の#resize()#呼び出しの際の#n#を$#n#_i$とする。
$r$を#resize()#の呼び出し回数とする。
このとき、#add(i,x)#と#remove(i)#の合計呼び出し回数は次の関係を満たす。
\[
  \sum_{i=1}^{r} (#n#_i/2-1) \le m \enspace
\]

これを変形すると次の式が得られる。
\[
  \sum_{i=1}^{r} #n#_i \le 2m + 2r  \enspace
\]

$r \leq m$より#resize()#呼び出しのための実行時間の合計は次のようになる。
\[
\sum_{i=1}^{r} O(#n#_i) \le O(m+r) = O(m)  \enspace
\]

あとは$(i-1)$から$i$回目の#resize()#の間に#add(i,x)#か#remove(i)#が呼ばれる回数が$#n#_i/2$以下であることを示す。

2つの場合が考えられる。
ひとつは#resize()#が#add(i,x)#に呼ばれる場合で、これはbacking arrayが一杯になるとき、つまり$#a.length# = #n#=#n#_i$が成り立つときだ。
直前の#resize()#を考えよう。この#resize()#の直後、#a#の大きさは#a.length#だが#a#の要素数は$#a.length#/2=#n#_i/2$以下であった。
しかし#a#の要素数は今では$#n#_i=#a.length#$なのだから、前の#resize()#から$#n#_i/2$回以上は#add(i,x)#が呼ばれたことがわかる。

もうひとつ考えられるのは、#resize()#が#remove(i)#に呼ばれる場合で、このとき$#a.length# \ge 3#n#=3#n#_i$である。
前の#resize()#の直後では#a#の要素数は$#a.length/2#-1$以下であった。
\footnote{この数式における${}-1$は、特別なケース$#n#=0$かつ$#a.length# = 1$を考慮したものだ。}
今#a#には$#n#_i\le#a.length#/3$個の要素が入っている。
よって、直前の#resize()#以降に実行された#remove(i)#の回数の下界は次のように計算できる。

  \begin{align*}
      R & \ge #a.length#/2 - 1 - #a.length#/3 \\
        & = #a.length#/6 - 1 \\
        & = (#a.length#/3)/2 - 1 \\
        & \ge #n#_i/2 -1\enspace .
  \end{align*}

いずれの場合でも、$(i-1)$から$i$回目の#resize()#の間に#add(i,x)#か#remove(i)#が呼ばれる回数の合計は$#n#_i/2-1$以上である。
\end{proof}

\subsection{要約}

次の定理は#ArrayStack#の性能を整理するものだ。

\begin{thm}\thmlabel{arraystack}
  #ArrayStack#は#List#インターフェースを実装する。
  #resize()#のコストを無視すると、#ArrayStack#における各操作の実行時間は、
  \begin{itemize}
    \item #get(i)#・#set(i,x)#の実行時間は$O(1)$である。
    \item #add(i,x)#・#remove(i)#の実行時間は$O(1+#n#-#i#)$である。
  \end{itemize}
  空の#ArrayStack#から任意の$m$個の#add(i,x)#・#remove(i)#からなる操作の列を実行する。このときすべての#resize()#にかかる時間の合計は$O(m)$である。
\end{thm}

#ArrayStack#は#Stack#を実装する効率的な方法である。
特に#push(x)#は#add(n,x)#、#pop()#は#remove(n-1)#のようにそれぞれ実装できる。
またいずれの操作も償却実行時間$O(1)$である。

\section{#FastArrayStack#：最適化されたArrayStack}

#ArrayStack#が主にやっていることは、データの（#add(i,x)#と#remove(i)#のための）シフトと（#resize()#のための）コピーである。

素朴な実装では#for#ループを使うだろう。
しかしデータのコピーや移動用の効率的な機能があるプログラミング環境も多いだろう。
C言語には#memcpy(d,s,n)#・#memmove(d,s,n)#関数がある。
C++言語には#std::copy(a0,a1,b)#アルゴリズムがある。
Javaには#System.arraycopy(s,i,d,j,n)#メソッドがある。
\index{memcpy@#memcpy(d,s,n)#}%
\index{std::copy@#std::copy(a0,a1,b)#}%
\index{System.arraycopy@#System.arraycopy(s,i,d,j,n)#}%

\cppimport{ods/FastArrayStack.add(i,x).remove(i).resize()}
\javaimport{ods/FastArrayStack.add(i,x).remove(i).resize()}

これらは最適化されており、#for#ループを使うよりかなり速くコピーができる特殊な機械命令を使うかもしれない。
これらを使っても漸近的な実行時間は減らないのだが、やる価値のある最適化ではある。

C++やJavaの実装で高速な配列コピーを使って2〜3倍の高速化できたこともある。どのくらい速くなるかは環境によるので是非試してみてほしい。

\section{ArrayQueue：配列を使ったキュー}
\seclabel{arrayqueue}

\index{ArrayQueue@#ArrayQueue#}%
この節ではFIFO（先入れ先出し）キューを実装するデータ構造#ArrayQueue#を紹介する。
(#add(x)#によって)要素が追加されたのと同じ順番で、（#remove()#によって）キューから要素が削除される。

FIFOキューの実装に#ArrayStack#はあまり向いていない。
一方の端から要素を追加し他方から要素を削除することになるので、賢明な選択ではないのだ。
2つの操作のうち一方はリストの先頭を変更することになる。つまり、$#i#=0$で#add(i,x)#か#remove(i)#を呼びだす。
このとき、#n#に比例する実行時間がかかってしまう。

配列を使ったキューの効率的な実装は、もし無限の配列#a#があれば簡単だろう。
次に削除する要素を追跡するインデックス#j#と、キューの要素数#n#を記録しておけばよい。
そうすればキューの要素は以下の場所に入っている。
\[ #a[j]#,#a[j+1]#,\ldots,#a[j+n-1]# \enspace \]
まず#i#, #j#を0に初期化する。
要素を追加するときは、#a[j+n]#に要素を入れて、#n#をひとつ増やす。
要素を削除するときは、#a[j]#から要素を取り出し、#j#をひとつ増やし、#n#をひとつ減らす。

もちろんこの方法の問題点は無限の配列が必要になることだ。
#ArrayQueue#は有限の配列#a#と\emph{剰余算術}で無限配列を模倣する。
\index{modular arithmetic}%
剰余算術は時間の計算をするときに使っているものだ。
例えば10:00に5時間を足すと3:00になる。
形式的に書けば次のようになる。
\[
    10 + 5 = 15 \equiv 3 \pmod{12} \enspace
\]
数式の後半は「12を法として15と3は合同である」と読む。
$\bmod$は次のように二項演算と考えてもよい。
\[
   15 \bmod 12 = 3 \enspace .
\]

より一般的には
整数$a$と正整数$m$について、ある整数kが存在し$a = r + km$をみたす$\{0、\ldots、m-1 \} $の一意な要素を$a \bmod m $と書く。
雑に言うと$ r $とは$ a $を$ m $で割った余りである。
CやC++、Ruby、Javaなど多くのプログラミング言語ではmod演算子は\%で表される。

剰余算術は無限配列を模倣するのに便利である。
$#i#\bmod #a.length#$は常に$0,\ldots,#a.length-1#$の値を取ることを利用して、配列の中にキューの要素をうまく入れられるのだ。

\[
#a[j%a.length]#,#a[(j+1)%a.length]#,\ldots,#a[(j+n-1)%a.length]# \enspace
\]

これは#a#を\emph{循環配列}として使っている。
\index{circular array}%
\index{array!circular}%
配列の添字が$#a.length#-1$を超えると、配列の先頭に戻ってくるのである。

残りの問題は、#ArrayQueue#の要素数が#a#の大きさを超えてはならないことだ。

\codeimport{ods/ArrayQueue.a.j.n}

#ArrayQueue#に対して#add(x)#・#remove()#からなる操作の列を実行する様子を\figref{arrayqueue}に示す。
#add(x)#はまず#a#が一杯かどうかを確認し、必要に応じて#resize()#を呼んで#a#の容量を増やす。
続いて#x#を#a[(j+n)%a.length]#に入れて、#n#をひとつ増やせばよい。

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/arrayqueue}
  \end{center}
  \caption[Adding and removing from an ArrayQueue]{A sequence of #add(x)# and #remove(i)# operations on an
  #ArrayQueue#.  Arrows denote elements being copied.  Operations that
  result in a call to #resize()# are marked with an asterisk.}
  \figlabel{arrayqueue}
\end{figure}

\codeimport{ods/ArrayQueue.add(x)}

最後になるが、#resize()#操作は#ArrayStack#のものとよく似ている。
大きさ$2#n#$の新しい配列#b#を割当て、
\[
   #a[j]#,#a[(j+1)%a.length]#,\ldots,#a[(j+n-1)%a.length]#
\]
を
\[
   #b[0]#,#b[1]#,\ldots,#b[n-1]#
\]
にコピーし、$#j#=0$とする。

\codeimport{ods/ArrayQueue.resize()}

\subsection{要約}

次の定理は#ArrayQueue#の性能を整理するものだ。

\begin{thm}
  #ArrayStack#は#List#インターフェースを実装する。
  #resize()#のコストを無視すると、#ArrayStack#における各操作の実行時間は、
  \begin{itemize}
    \item #get(i)#・#set(i,x)#の実行時間は$O(1)$である。
    \item #add(i,x)#・#remove(i)#の実行時間は$O(1+#n#-#i#)$である。
  \end{itemize}
  空の#ArrayStack#から任意の$m$個の#add(i,x)#・#remove(i)#からなる操作の列を実行する。このときすべての#resize()#にかかる時間の合計は$O(m)$である。
  #ArrayQueue#は(FIFO)#Queue#インターフェースの実装である。
  #resize()#のコストを無視すると、#ArrayStack#は#add(x)#・#remove()#を$O(1)$の時間で実行できる。
  さらに、空の#ArrayStack#に対して長さ#m#の任意の#add(i,x)#・#remove(i)#からなる操作の列を実行するとき、#resize()#に使われる実行時間の合計は$O(m)$である。
\end{thm}

\section{ArrayDeque：配列を使った高速な双方向キュー}
\seclabel{arraydeque}

\index{ArrayDeque@#ArrayDeque#}%
%XXX: ここまで
前節の#ArrayQueue#は、一方の端からは追加が他方の端から削除が効率的にできる列を表現するデータ構造だった。#ArrayDeque#は両方の端で効率的な追加と削除ができるデータ構造である。
#ArrayQueue#を表現するために使った循環配列をここでもまた使って#List#インタフェースを実装する。

\codeimport{ods/ArrayDeque.a.j.n}

#ArrayDeque#における#get(i)#と#set(i,x)#の実装は難しくない。
配列の要素$#a[#{#(j+i)#\bmod #a.length#}#]#$を読み書きすればよいのだ。

\codeimport{ods/ArrayDeque.get(i).set(i,x)}

#add(i,x)#の実装はもう少し興味深い。
まず、#a#が一杯かどうかを確認し、必要に応じて#resize()#を呼ぶ。
ここで、#i#が小さいとき（0に近いとき）と大きいとき（#n#に近いとき）に、特に効率的に操作したいのだということを覚えておいてほしい。
つづいて、$#i#<#n#/2$かどうかを確認する。
もしそうなら、$#a[0]#,\ldots,#a[i-1]#$をそれぞれひとつずつ左にずらす。
そうでないなら、$#a[i]#,\ldots,#a[n-1]#$をそれぞれひとつずつ右にずらす。
#add(i,x)#と#remove(x)#の説明として\figref{arraydeque}を見てほしい。

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/arraydeque}
  \end{center}
  \caption[Adding and removing from an ArrayDeque]{A sequence of #add(i,x)# and #remove(i)# operations on an
  #ArrayDeque#.  Arrows denote elements being copied.}
  \figlabel{arraydeque}
\end{figure}

\codeimport{ods/ArrayDeque.add(i,x)}

このようにシフトを行うことで、#add(i,x)#は$\min\{ #i#, #n#-#i# \}$より多くの要素をシフトしなくてよい。
よって#add(i,x)#の（#resize()#のことを無視した）実行時間は$O(1+\min\{#i#,#n#-#i#\})$である。

#remove(i)#の実装も似たようなものだ。
$#a[0]#,\ldots,#a[i-1]#$をそれぞれ右にひとつずつシフトするか、$#a[i+1]#,\ldots,#a[n-1]#$をそれぞれ左にひとつずつシフトするか、$#i#<#n#/2$かどうかに応じてどちらかを行う。
やはり#remove(i)#も$O(1+\min\{#i#,#n#-#i#\})$だけの時間で要素を動かし終えることができる。

\codeimport{ods/ArrayDeque.remove(i)}

\subsection{要約}

次の定理は#ArrayDeque#の性能を整理するものだ。
  #ArrayDeque#は#List#インターフェースを実装する。
  #resize()#のコストを無視すると、#ArrayDeque#における各操作の実行時間は、
  \begin{itemize}
    \item #get(i)#・#set(i,x)#の実行時間は$O(1)$である。
    \item #add(i,x)#・#remove(i)#の実行時間は$O(1+\min\{#i#,#n#-#i#\})$である。
  \end{itemize}
  空の#ArrayDeque#から任意の$m$個の#add(i,x)#・#remove(i)#からなる操作の列を実行する。このときすべての#resize()#にかかる時間の合計は$O(m)$である。
  #ArrayQueue#は(FIFO)#Queue#インターフェースの実装である。
  #resize()#のコストを無視すると、#ArrayDeque#は#add(x)#・#remove()#を$O(1)$の時間で実行できる。
  さらに、空の#ArrayDeque#に対して長さ#m#の任意の#add(i,x)#・#remove(i)#からなる操作の列を実行するとき、#resize()#に使われる実行時間の合計は$O(m)$である。
