\chapter{Introduction}
\pagenumbering{arabic}

世界中のコンピュータサイエンスカリキュラムにデータ構造とアルゴリズムに関するコースが含まれている。データ構造は重要だ。生活の質を上げ、日常生活を効率的にしてくれる。数百万ドル、数十億ドルの企業にはデータ構造を中心に作られたものも多い。

どういうものなのだろう？深く考えるのをやめると、データ構造と普段から接していることに気づく。
\begin{itemize}
	\item ファイルを開く：ファイルシステム\index{file system}のデータ構造を使って、ファイルをディスク上に配置し、検索できる。これは簡単ではない。ディスクは数億のブロックを含む。ファイルの内容はそのどこかに保存されるのだ。
	\item 連絡先を検索する：ダイヤル途中の部分的な情報にもとづき、連絡先リスト\index{contact list}から電話番号を見つけるためにデータ構造が使われる。これは簡単ではない。連絡先リストに含まれる情報はとても多いかもしれない。これまで電話や電子メールで連絡したことのある全ての人を想像してみてほしい。また、電話のプロセッサはあまり高性能でなく、メモリも潤沢でない。
	\item SNSにログインする：\index{social network}ネットワークサーバーは、ログイン情報からアカウント情報を検索する。これは簡単ではない。人気のソーシャルネットワークには何億人ものアクティブユーザーがいる。
	\item Webページを検索する：\index{web search}検索エンジンは検索語を含むWebページを見つけるためにデータ構造を使う。これは簡単ではない。インターネットには85億以上のWebページがあり、個々のページには検索されるかもしれない単語が多く含まれている。
	\item 緊急サービス（9-1-1）に電話する：\index{emergency services}\index{9-1-1}緊急サービスネットワークはパトカー、救急車、消防車が速やかに現場に到着できるよう、電話番号と住所を対応づけるデータ構造を使う。これは重要だ。電話をかけた人は正確な住所を伝えられないかもしれず、遅れが生死を別つ可能性があるためである。
\end{itemize}

\section{効率の必要性}

次節では多くのデータ構造がサポートする操作を見ていく。ちょっとしたプログラミング経験があれば、これらを正しく実装することは難しくない。データを配列または線形リストに格納し、配列または線形リストの全要素を見てまわり、要素を追加したり削除したりすればよいのだ。

こういう実装は簡単だが、あまり効率的ではない。さて、このことを考える価値はあるだろうか？コンピュータはどんどん高速化している。自明な実装で十分だろう。確認のためにざっくりとした計算をしてみよう。

\paragraph{操作の数：}
まあまあの大きさのデータセット、例えば100万（$10^6$）のアイテムを持つアプリケーションがあるとする。少なくとも一度は各アイテムを参照すると仮定してよいことが多いだろう。つまり少なくとも100万（$10^6$）回はこのデータで検索をしたいわけだ。この$10^6$回の検索それぞれが$10^6$個のアイテムをすべて確認すると、合計$10^6\times 10^6=10^{12}$（1000億）回の確認処理が必要だ。

\paragraph{プロセッサの速度：}
本書の執筆の時点で、かなり高速なデスクトップコンピュータでも毎秒10億（$10^9$）の操作は実行できない。\footnote{コンピュータの速度はせいぜい数ギガヘルツ（数十億回/秒）であり、各操作にふつう数サイクルが必要だ。}よってこのアプリケーションの完了には少なくとも$10^{12}/10^9=1000$秒、すなわち約16分40秒かかる。コンピューターの時間では16分は非常に長いが、人は気にしないかもしれない。（例えば、プログラマがコーヒーブレイクに向かうならそれで構わないだろう。）

\paragraph{大きなデータセット：}
Google\index{Google|}を考えてみよう。Googleでは85億ものWebページからの検索を扱う。先ほどの計算では、このデータに対するクエリは少なくとも8.5秒かかる。だがこれが事実ではないことはわかるだろう。Web検索には8.5秒もかからないし、あるページがインデックスに含まれているかよりさらに複雑なクエリを実行する。執筆時点でGoogleは1秒間に約$4,500$クエリを受けつける。つまり少なくとも$4,500 \times 8.5 = 38,250$ものサーバーが必要なのだ。

\paragraph{解決策：}
以上の例から、アイテム数$\mathtt{n}$と実行される操作数$m$が共に大きくなると、データ構造の自明な実装はスケールしないことがわかる。今の例で、（機械命令数で数えた）時間はおよそ$#n#\times m$だ。

解決策はもちろん、データ構造内のデータを注意深く格納して、各操作がすべてのデータを見て回る必要がないようにすることだ。最初はムリなように思えるが、データ構造に格納されているデータの数とは関係なく、平均して2つのデータだけを参照すればすむデータ構造をのちに紹介する。 1秒あたり10億命令を実行できるとして、10億個のデータ（兆、京、垓であっても）を含むデータ構造を検索するのにわずか$0.000000002$秒しかかからないのだ。

データ構造内のデータを整列することで、データ数に対して参照されるデータ数が非常にゆっくり増加するデータ構造も後で紹介する。例えば10億個のアイテムを整列しておけば、最大60個のアイテムを参照することで各操作を実行できる。毎秒10億命令実行できるコンピュータでは、これらの操作にそれぞれ$0.00000006$秒かかる。

この章の残りの部分では、この本の残りの部分で使用される主な概念の一部を簡単に解説する。\secref{interface}は本書で説明する全データ構造で実装されるインターフェースを説明するので読む必要がある。残りの節では、以下のものを解説する。
\begin{itemize}
\item 指数・対数・階乗関数や漸近(ビッグオー)記法、確率、ランダム化などの数学の復習
\item 計算モデル
\item 正しさと実行時間、メモリ使用量
\item 残りの章の概要
\item サンプルコードと組版の規則
\end{itemize}
これらの背景知識があってもなくても、いったん気軽に飛ばし必要に応じて戻り読みしてもよい。

\section{インターフェイス}
\seclabel{インターフェイス}
データ構造について議論するときは、データ構造のインターフェースと実装の違いを理解することが重要だ。インターフェースはデータ構造が何をするかを、実装はデータ構造がどうやるかを示す。

\emph{インターフェース}\index{interface}\index{abstract data type|see{interface}}（\emph{抽象データ型}と呼ばれることもある）は、データ構造がサポートする操作一式とその意味を定義する。インターフェースを見ても操作がどう実装されているかはわからない。サポートする操作の一覧とその引数、返り値の特徴だけを教えてくれる。

一方でデータ構造の\emph{実装}には、データ構造の内部表現と、操作を実装するアルゴリズムの定義が含まれる。そのため、あるインターフェースに対する複数の実装が考えられる。例えば、\chapref{arrays}では配列を、\chapref{linkedlists}ではポインタを使って#List#インターフェースを実装する。それぞれ同じ#List#インターフェースを実装しているが、実装方法は異なるのだ。

\subsection{#Queue#、#Stack#、#Deque#インターフェース}

#Queue#インターフェースは要素を追加したり、次の要素を削除したりできる要素の集まりを表す。より正確には、#Queue#インターフェースがサポートする操作は以下のものだ。

\begin{itemize}
  \item #add(x)#：値#x#を#Queue#に追加する。
  \item #remove()#：（以前に追加された）次の値#y#を#Queue#から削除し、#y#を返す。
\end{itemize}

#remove()#操作の引数はないことに注意する。#Queue#は\emph{取り出し規則}に従って削除する要素を決める。取り出し規則は色々と考えられるが、主なものとしてはFIFOや優先度、LIFOなどがある。

\figref{queue}に示す\emph{FIFO（first-in-first-out、先入れ先出し）#Queue#}\index{FIFO queue} \index{queue!FIFO}は、追加したのと同じ順番で要素を削除する。これは食料品店のレジで並ぶ列と同じようなものだ。これは最も一般的な#Queue#なので、修飾子のFIFOは省略されることが多い。他の教科書ではFIFO #Queue#の#add(x)#、#remove()#は、それぞれ#enqueue(x)#、#dequeue()#と呼ばれていることも多い。

\begin{figure}
  \centering{\includegraphics[width=\ScaleIfNeeded]{figs/queue}}
  \caption[FIFOキュー]{FIFO #Queue#.}
  \figlabel{queue}
\end{figure}

\fig{prioqueue}に示す優先度付き#Queue#は、
\index{priority queue}%
\index{priority queue|seealso{heap}}%
\index{queue!priority}%
#Queue#から最小の要素を削除します。同点要素が複数あるときは、そのうちのいずれかを適当に選ぶ。これは病院の救急室で重症患者を優先的に治療することに似ている。患者が到着したら、症状を見積もってから待合室で待機していてもらう。医師の手が空くと、最も重篤な患者から治療する。優先度付き#Queue#における#remove()#操作を、他の教科書ではよく#deleteMin()#などと呼んでいる。

\begin{figure}
  \centering{\includegraphics[width=\ScaleIfNeeded]{figs/prioqueue}}
  \caption[優先度付きキュー]{A priority #Queue#.}
  \figlabel{prioqueue}
\end{figure}

よく使う取り出し規則は、図1.3に示すLIFO（last-in-first-out、後入れ先出し）
\index{LIFO queue}%
\index{LIFO queue|seealso{stack}}%
\index{queue!LIFO}%
\index{stack}%
だ。 \emph{LIFOキュー}では、最後に追加された要素が次に削除される。これはプレートを積むように視覚化するとよい。プレートはスタックの上に積まれ、上から順に持って行かれる。この構造はとてもよく見かけるので#Stack#という名前が付いている。#Stack#について話すとき、#add(x)#、#remove()#のことを、#push(x)#、#pop()#と呼ぶ。こうすればLIFOとFIFOの取り出し規則を区別できる。

\begin{figure}
  \centering{\includegraphics[width=\ScaleIfNeeded]{figs/stack}}
  \caption[スタック]{スタック}
  \figlabel{stack}
\end{figure}

#Deque#（双方向キュー）はFIFOキューとLIFOキュー（スタック）の一般化だ。 #Deque#は前と後ろのある要素の列を表す。列の前または後ろに要素を追加できる。 #Deque#操作の名前はわかりやすく、#addFirst(x)#、#removeFirst()#、#addLast(x)#、 #removeLast()#だ。スタックは#addFirst()#と#removeFirst()#だけを使って実装できる。一方FIFOキューには#addLast(x)#と#removeFirst()#を使えばよい。

% XXX: ここまで完了
\subsection{#List#インターフェース：線形シーケンス}

この本は、FIFO #Queue#、#Stack#、または#Deque#インターフェースについてはほとんど話をしません。これは、これらのインターフェースが#List#インターフェースに組み込まれているためです。図1.4に示すA #List#は、シーケンスを表します。$ \ensuremath {\mathtt{x}} _ 0、\ldots、\ensuremath {\mathtt{x}} _ {\suremath {\mathtt{ n}} - 1} $の値を返します。 $ \mathtt{List} $インターフェイスには以下の操作が含まれています：

$ \mathtt{size（）} $：リストの長さを返す$ \mathtt{n} $
$ \mathtt{get（i）} $：値を返す$ \ensuremath {\mathtt{x}} _ {\suremath {\mathtt{i}}} $
$ \mathtt{$ mathtt{i}}} $} $の値を$ \mathtt{x} $と等しく設定する$ \mathtt{set（i、x）} $：
$ \mathtt{add（i、x）} $：$ \mathtt{i} $の位置に$ \mathtt{x} i}}}、\ldots、\suremath {\mathtt{x}} _ {\mathtt{n}} - 1} $;
\{\suremath {\mathtt{n}} - 1のすべての$ j \に対して、
\ensuremath {\mathtt{x}} $ {$ mathtt{x}} $と置き換えて、$ \mathtt{remove {i}} $を削除します。 \suremath {\mathtt{1}}、\ldots、\ensuremath {
	\{\suremath {\mathtt{i}}内のすべての$ j \に対して、$ \ensuremath {\mathtt{x}} _ {j} = \、\ldots、\suremath {\mathtt{n}} - 2 \} $と減算$ \mathtt{n} $

		これらの操作は、$ \mathtt{Deque} $インターフェースを実装するのには簡単に十分です：
		$ \displaystyle \ensuremath {\mathtt{add（$ x）}}} $ $ \displaystyle \
			$ \displaystyle \ensuremath {\mathtt{remove（0）}} $ \displaystyle \ensuremath {\mathtt{remove（0）}} $
			$ \displaystyle \ensuremath {\mathtt{add（x）}} $ $ \displaystyle \rightmath {\mathtt{add（size、）、x）}} $
			$ \displaystyle \ensuremath {\mathtt{remove（size - ） - 1}}} $ $ \displaystyle \

			図1.4：A $ \mathtt{List} $は$ 0,1,2、\ldots、\ensuremath {\mathtt{n}} - 1 $で索引付けされたシーケンスを表します。この$ \mathtt{List} $で、$ \mathtt{get（2）} $への呼び出しは値$ c $を返します。
			\includegraphics [width = \textwidth] {figs / list}

			次の章では通常、$ \mathtt{Stack} $、$ \mathtt{Deque} $、FIFO $ \mathtt{Queue} $インターフェースについては議論しませんが、$ \mathtt{Stack} $と$ \mathtt{ Deque} $は、$ \mathtt{List} $インターフェースを実装するデータ構造体の名前で使用されることがあります。これが起きると、これらのデータ構造を使って$ \mathtt{Stack} $や$ \mathtt{Deque} $インターフェースを非常に効率的に実装することができます。たとえば、$ \mathtt{ArrayDeque} $クラスは、すべての$ \mathtt{Deque} $操作を1回の操作で一定時間に実装する$ \mathtt{List} $インターフェースの実装です。

			1.2.3 $ \mathtt{USet} $インターフェース：順序付けられていないセット

			$ \mathtt{USet} $インターフェースは、数学的な集合を模倣する一意の要素の順序付けられていない集合を表します。 A $ \mathtt{USet} $には、$ \mathtt{n} $個の要素が含まれています。要素が複数回出現することはありません。要素は特定の順序ではありません。 A $ \mathtt{USet} $は次の操作をサポートしています：

			$ \mathtt{size（）} $：集合内の要素の数$ $ mathtt{n} $を返す
			$ \mathtt{add（x）} $：要素$ \mathtt{x} $をまだセットしていない場合はその集合に追加します。
			$ \mathtt{x} $が$ \mathtt{y} $と等しくなるような集合の中に要素$ \mathtt{y} $がないならば、集合に$ \mathtt{x} $を加える。 $ \mathtt{x} $がセットに追加された場合は$ \mathtt{true} $を返し、それ以外の場合は$ \mathtt{false} $を返します。
			$ \mathtt{remove（x）} $：セットから$ \mathtt{x} $を削除する。
			$ \mathtt{x} $が$ \mathtt{y} $と等しく、$ \mathtt{y} $を取り除くような集合の要素$ \mathtt{y} $を探します。そのような要素が存在しない場合、$ \mathtt{y} $、または$ \mathtt{null} $を返します。
			$ \mathtt{find（x）} $：セット内に$ \mathtt{x} $があればそれを見つけます。
			集合中の$ \mathtt{y} $が$ \mathtt{x} $と等しくなるような要素$ \mathtt{y} $を見つける。そのような要素が存在しない場合、$ \mathtt{y} $、または$ \mathtt{null} $を返します。

			これらの定義は、$ \mathtt{y} $から削除または見つけられる要素である$ \mathtt{x} $を、我々が削除したり見つけたりする可能性のある要素と区別することに少し難解です。これは、$ \mathtt{x} $と$ \mathtt{y} $は実際には等しく扱われる別個のオブジェクトであるかもしれないからです。このような区別は、キーを値にマップする辞書やマップを作成できるので便利です。

			辞書/マップを作成するには、$ \mathtt{Pair} $ sという複合オブジェクトを作成します。それぞれのオブジェクトにはキーと値が含まれています。 2つの$ \mathtt{Pair} $ sは、それらのキーが等しい場合、等しいとみなされます。 $ \mathtt{USET} $にある$ \mathtt{find（x）}を呼び出すと、$ \mathtt{$ mathtt{ $ \ensuremath {\mathtt{$ mathtt{}} $ \ensuremath {\mathtt{$ mathtt{}} $ \y}} =（\suremath {\mathtt{k}}、\suremath {\mathtt{v}}）$。言い換えれば、キー$ \mathtt{k} $のみが与えられた場合、$ \mathtt{v} $という値を復元することができます。


				1.2.4 $ \mathtt{SSet} $インターフェース：ソートされたセット

					$ \mathtt{SSet} $インターフェースはソートされた要素の集合を表します。 $ \mathtt{SS} $は、何らかの総順序から要素を格納するので、任意の2つの要素$ \mathtt{x} $と$ \mathtt{y} $を比較することができます。コード例では、これは$ \mathtt{compare（x、y）} $というメソッドで行われます。

					$ \{\mathtt{compare（x、y）}} \begin {cases} {} <0＆\text {... ... {} = 0＆ mathtt{x}} = \suremath {\mathtt{y}} $} \end {cases} $

					$ \mathtt{SSET} $は、以下のような全く同じセマンティクスを持つ$ \mathtt{size（）} $、$ \mathtt{add（x）} $、$ \mathtt{remove（x）} $メソッドをサポートします。 $ \mathtt{USet} $インターフェイス。 $ \mathtt{USet} $と$ \mathtt{SSet} $の違いは$ \mathtt{find（x）} $メソッドの違いです：

					$ \mathtt{find（x）} $：ソートされた集合の中で$ \mathtt{x} $を見つける。
					$ \ensuremath {\mathtt{y}} \ge \ensuremath {\mathtt{x}} $のように、最小の要素$ \mathtt{y} $を検索します。そのような要素が存在しない場合、$ \mathtt{y} $または$ \mathtt{null} $を返します。

					このバージョンの$ \mathtt{find（x）} $操作は、後継検索と呼ばれることがあります。 $ \mathtt{x} $に等しい要素がない場合でも意味のある結果を返すので、基本的な方法は$ \mathtt{USet.find（x）} $と異なります。

					$ \mathtt{USet} $と$ \mathtt{SSet} $ $ \mathtt{find（x）} $演算の区別は非常に重要であり、しばしば見逃されます。 $ \mathtt{SSet} $によって提供される特別な機能には通常、実行時間が長く、実装の複雑性が高いという価格が含まれています。例えば、この本で議論されている$ \mathtt{SSet} $実装の大部分はすべて、実行時に$ \mathtt{find（x）} $演算を持ち、これは対数の対数である。一方、第5章の$ \mathtt{ChainedHashTable} $としての$ \mathtt{USet} $の実装には、一定の予想時間内に実行される$ \mathtt{find（x）} $演算があります。これらの構造のどれを使用するかを選択するときは、$ \mathtt{SSet} $によって提供される特別な機能が本当に必要でない限り、常に$ \mathtt{USet} $を使用する必要があります。

					\section{Mathematical Background}

					In this section, we review some mathematical notations and tools used throughout this book, including logarithms, big-Oh notation, and probability theory.  This review will be brief and is not intended as an introduction. Readers who feel they are missing this background are encouraged to read, and do exercises from, the appropriate sections of the very good (and free) textbook on mathematics for computer science \cite{llm11}.

					\subsection{Exponentials and Logarithms}

					\index{exponential}
					The expression $b^x$ denotes the number $b$ raised to the power of $x$.  If $x$ is a positive integer, then this is just the value of $b$ multiplied by itself $x-1$ times:
					\[
					b^x = \underbrace{b\times b\times \cdots \times b}_{x} \enspace .
					\]
					When $x$ is a negative integer, $b^{-x}=1/b^{x}$.  When $x=0$, $b^x=1$.  When $b$ is not an integer, we can still define exponentiation in terms of the exponential function $e^x$ (see below), which is itself defined in terms of the exponential series, but this is best left to a calculus text.

					\index{logarithm}
					In this book, the expression $\log_b k$ denotes the \emph{base-$b$ logarithm} of $k$.  That is, the unique value $x$ that satisfies
					\[
					b^{x} = k  \enspace .
\]
Most of the logarithms in this book are base 2 (\emph{binary logarithms}).  \index{binary logarithm} \index{logarithm!binary} For these, we omit the base, so that $\log k$ is shorthand for $\log_2 k$.

An informal, but useful, way to think about logarithms is to think of $\log_b k$ as the number of times we have to divide $k$ by $b$ before the result is less than or equal to 1.  For example, when one does binary search, each comparison reduces the number of possible answers by a factor of 2.  This is repeated until there is at most one possible answer.  Therefore, the number of comparison done by binary search when there are initially at most $n+1$ possible answers is at most $\lceil\log_2(n+1)\rceil$.

\index{natural logarithm}
\index{logarithm!natural}
Another logarithm that comes up several times in this book is the \emph{natural logarithm}.  Here we use the notation $\ln k$ to denote $\log_e k$, where $e$ --- \emph{Euler's constant} --- is given by \index{Euler's constant} \index{e@$e$ (Euler's constant)}
\[
   e = \lim_{n\rightarrow\infty} \left(1+\frac{1}{n}\right)^n
   \approx  2.71828 \enspace .
\]
The natural logarithm comes up frequently because it is the value of a particularly common integral:
\[
    \int_{1}^{k} 1/x\,\mathrm{d}x  = \ln k \enspace .
\]
Two of the most common manipulations we do with logarithms are removing them from an exponent:
\[
    b^{\log_b k} = k
\]
and changing the base of a logarithm:
\[
    \log_b k = \frac{\log_a k}{\log_a b} \enspace .
\]
For example, we can use these two manipulations to compare the natural and binary logarithms
\[
   \ln k = \frac{\log k}{\log e} = \frac{\log k}{(\ln e)/(\ln 2)} = 
    (\ln 2)(\log k) \approx 0.693147\log k \enspace .
\]

\subsection{Factorials}
\seclabel{factorials}

\index{factorial}
In one or two places in this book, the \emph{factorial} function is used.  For a non-negative integer $n$, the notation $n!$ (pronounced ``$n$ factorial'') is defined to mean 
\[
   n! = 1\cdot2\cdot3\cdot\cdots\cdot n \enspace .
\]
Factorials appear because $n!$ counts the number of distinct permutations, i.e., orderings, of $n$ distinct elements.  \index{permutation} For the special case $n=0$, $0!$ is defined as 1. 

\index{Stirling's Approximation} The quantity $n!$ can be approximated using \emph{Stirling's Approximation}:
\[
	n! 
   = \sqrt{2\pi n}\left(\frac{n}{e}\right)^{n}e^{\alpha(n)} \enspace ,
\]
where
\[  
   \frac{1}{12n+1} <  \alpha(n) < \frac{1}{12n}  \enspace .
\]
Stirling's Approximation also approximates $\ln(n!)$:
\[
   \ln(n!) = n\ln n - n + \frac{1}{2}\ln(2\pi n) + \alpha(n)
\]
(In fact, Stirling's Approximation is most easily proven by approximating
$\ln(n!)=\ln 1 + \ln 2  + \cdots + \ln n$ by the integral
$\int_1^n \ln n\,\mathrm{d}n = n\ln n - n +1$.)

\index{binomial coefficients}
Related to the factorial function are the \emph{binomial coefficients}.  For a non-negative integer $n$ and an integer $k\in\{0,\ldots,n\}$, the notation $\binom{n}{k}$ denotes:
\[
   \binom{n}{k} = \frac{n!}{k!(n-k)!} \enspace .
\]
The binomial coefficient $\binom{n}{k}$ (pronounced ``$n$ choose $k$'') counts the number of subsets of an $n$ element set that have size $k$, i.e., the number of ways of choosing $k$ distinct integers from the set $\{1,\ldots,n\}$.

\subsection{Asymptotic Notation}

\index{asymptotic notation} \index{big-Oh notation} \index{O@$O$ notation} When analyzing data structures in this book, we want to talk about the running times of various operations.  The exact running times will, of course, vary from computer to computer and even from run to run on an individual computer.  When we talk about the running time of an operation we are referring to the number of computer instructions performed during the operation.  Even for simple code, this quantity can be difficult to compute exactly.  Therefore, instead of analyzing running times exactly, we will use the so-called \emph{big-Oh notation}: For a function $f(n)$, $O(f(n))$ denotes a set of functions,
\[
   O(f(n)) = \left\{
     \begin{array}{l}
       g(n):\mbox{there exists $c>0$, and $n_0$ such that} \\
             \quad\mbox{$g(n) \le c\cdot f(n)$ for all $n\ge n_0$}   
     \end{array} \right\} \enspace .
\]
Thinking graphically, this set consists of the functions $g(n)$ where $c\cdot f(n)$ starts to dominate $g(n)$ when $n$ is sufficiently large.

We generally use asymptotic notation to simplify functions.  For example, in place of $5n\log n + 8n - 200$ we can write $O(n\log n)$.
This is proven as follows:
\begin{align*} 
       5n\log n + 8n - 200
        & \le 5n\log n + 8n \\
        & \le 5n\log n + 8n\log n & \mbox{ for $n\ge 2$ (so that $\log n \ge 1$)}
            \\
        & \le 13n\log n  \enspace .
\end{align*}
This demonstrates that the function $f(n)=5n\log n + 8n - 200$ is in the set $O(n\log n)$ using the constants $c=13$ and $n_0 = 2$.

A number of useful shortcuts can be applied when using asymptotic notation.  First:
\[ O(n^{c_1}) \subset O(n^{c_2}) \enspace ,\]
for any $c_1 < c_2$.  Second: For any constants $a,b,c > 0$,
\[ O(a) \subset O(\log n) \subset O(n^{b}) \subset O({c}^n) \enspace . \]
These inclusion relations can be multiplied by any positive value, and they still hold. For example, multiplying by $n$ yields:
\[ O(n) \subset O(n\log n) \subset O(n^{1+b}) \subset O(n{c}^n) \enspace . \]

Continuing in a long and distinguished tradition, we will abuse this notation by writing things like $f_1(n) = O(f(n))$ when what we really mean is $f_1(n) \in O(f(n))$.  We will also make statements like ``the running time of this operation is $O(f(n))$'' when this statement should be ``the running time of this operation is \emph{a member of} $O(f(n))$.'' These shortcuts are mainly to avoid awkward language and to make it easier to use asymptotic notation within strings of equations.

A particularly strange example of this occurs when we write statements like
\[
  T(n) = 2\log n + O(1)  \enspace .
\]
Again, this would be more correctly written as
\[
  T(n) \le 2\log n + [\mbox{some member of $O(1)$]}  \enspace .
\]

The expression $O(1)$ also brings up another issue. Since there is no variable in this expression, it may not be clear which variable is getting arbitrarily large.  Without context, there is no way to tell.  In the example above, since the only variable in the rest of the equation is $n$, we can assume that this should be read as $T(n) = 2\log n + O(f(n))$, where $f(n) = 1$.

Big-Oh notation is not new or unique to computer science.  It was used by the number theorist Paul Bachmann as early as 1894, and is immensely useful for describing the running times of computer algorithms.  Consider the following piece of code: \javaimport{junk/Simple.snippet()} \cppimport{ods/Simple.snippet()} One execution of this method involves
\begin{itemize}
      \item $1$ assignment (#int\, i\, =\, 0#),
      \item $#n#+1$ comparisons (#i < n#),
      \item #n# increments (#i++#),
      \item #n# array offset calculations (#a[i]#), and
      \item #n# indirect assignments (#a[i] = i#).
\end{itemize}
So we could write this running time as
\[
    T(#n#)=a + b(#n#+1) + c#n# + d#n# + e#n# \enspace , 
\]
where $a$, $b$, $c$, $d$, and $e$ are constants that depend on the machine running the code and represent the time to perform assignments, comparisons, increment operations, array offset calculations, and indirect assignments, respectively.  However, if this expression represents the running time of two lines of code, then clearly this kind of analysis will not be tractable to complicated code or algorithms.  Using big-Oh notation, the running time can be simplified to
\[
    T(#n#)= O(#n#) \enspace .
\]
Not only is this more compact, but it also gives nearly as much information.  The fact that the running time depends on the constants $a$, $b$, $c$, $d$, and $e$ in the above example means that, in general, it will not be possible to compare two running times to know which is faster without knowing the values of these constants.  Even if we make the effort to determine these constants (say, through timing tests), then our conclusion will only be valid for the machine we run our tests on.

Big-Oh notation allows us to reason at a much higher level, making it possible to analyze more complicated functions.  If two algorithms have the same big-Oh running time, then we won't know which is faster, and there may not be a clear winner.  One may be faster on one machine, and the other may be faster on a different machine.  However, if the two algorithms have demonstrably different big-Oh running times, then we can be certain that the one with the smaller running time will be faster \emph{for large enough values of #n#}.

An example of how big-Oh notation allows us to compare two different functions is shown in \figref{intro-asymptotics}, which compares the rate of growth of $f_1(#n#)=15#n#$ versus $f_2(n)=2#n#\log#n#$.  It might be that $f_1(n)$  is the running time of a complicated linear time algorithm while $f_2(n)$ is the running time of a considerably simpler algorithm based on the divide-and-conquer paradigm.  This illustrates that, although $f_1(#n#)$ is greater than $f_2(n)$ for small values of #n#, the opposite is true for large values of #n#.  Eventually $f_1(#n#)$ wins out, by an increasingly wide margin.  Analysis using big-Oh notation told us that this would happen, since $O(#n#)\subset O(#n#\log #n#)$.

\begin{figure}
  \begin{center}
    \newlength{\tmpa}\setlength{\tmpa}{.98\linewidth}
    \addtolength{\tmpa}{-4mm}
    \resizebox{\tmpa}{!}{\input{images/bigoh-1.tex}}\\[4ex]
    \resizebox{.98\linewidth}{!}{\input{images/bigoh-2.tex}}
  \end{center}
  \caption{Plots of $15#n#$ versus $2#n#\log#n#$.}
  \figlabel{intro-asymptotics}
\end{figure}

In a few cases, we will use asymptotic notation on functions with more than one variable. There seems to be no standard for this, but for our purposes, the following definition is sufficient:
\[
   O(f(n_1,\ldots,n_k)) = 
   \left\{\begin{array}{@{}l@{}}
             g(n_1,\ldots,n_k):\mbox{there exists $c>0$, and $z$ such that} \\
             \quad \mbox{$g(n_1,\ldots,n_k) \le c\cdot f(n_1,\ldots,n_k)$} \\
             \qquad \mbox{for all $n_1,\ldots,n_k$ such that $g(n_1,\ldots,n_k)\ge z$}   
   \end{array}\right\} \enspace .
\]
This definition captures the situation we really care about:  when the arguments $n_1,\ldots,n_k$ make $g$ take on large values.  This definition also agrees with the univariate definition of $O(f(n))$ when $f(n)$ is an increasing function of $n$.  The reader should be warned that, although this works for our purposes, other texts may treat multivariate functions and asymptotic notation differently.


\subsection{Randomization and Probability}
\seclabel{randomization}

\index{randomization}
\index{probability}
\index{randomized data structure}
\index{randomized algorithm}
Some of the data structures presented in this book are \emph{randomized}; they make random choices that are independent of the data being stored in them or the operations being performed on them.  For this reason, performing the same set of operations more than once using these structures could result in different running times.  When analyzing these data structures we are interested in their average or \emph{expected} running times.
\index{expected running time}
\index{running time!expected}

Formally, the running time of an operation on a randomized data structure is a random variable, and we want to study its \emph{expected value}.  \index{expected value} For a discrete random variable $X$ taking on values in some countable universe $U$, the expected value of $X$, denoted by $\E[X]$, is given by the formula
\[
    \E[X] = \sum_{x\in U} x\cdot\Pr\{X=x\} \enspace .
\]
Here $\Pr\{\mathcal{E}\}$ denotes the probability that the event $\mathcal{E}$ occurs.  In all of the examples in this book, these probabilities are only with respect to the random choices made by the randomized data structure;  there is no assumption that the data stored in the structure, nor the sequence of operations performed on the data structure, is random.

One of the most important properties of expected values is \emph{linearity of expectation}.  \index{linearity of expectation} For any two random variables $X$ and $Y$,
\[
   \E[X+Y] = \E[X] + \E[Y] \enspace .
\]
More generally, for any random variables $X_1,\ldots,X_k$,
\[
   \E\left[\sum_{i=1}^k X_i\right] = \sum_{i=1}^k \E[X_i] \enspace .
\]
Linearity of expectation allows us to break down complicated random variables (like the left hand sides of the above equations) into sums of simpler random variables (the right hand sides).

A useful trick, that we will use repeatedly, is defining \emph{indicator random variables}.  \index{indicator random variable} These binary variables are useful when we want to count something and are best illustrated by an example.  Suppose we toss a fair coin $k$ times and we want to know the expected number of times the coin turns up as heads.  \index{coin toss} Intuitively, we know the answer is $k/2$, but if we try to prove it using the definition of expected value, we get
\begin{align*}
   \E[X] & = \sum_{i=0}^k i\cdot\Pr\{X=i\} \\
         & = \sum_{i=0}^k i\cdot\binom{k}{i}/2^k \\
         & = k\cdot \sum_{i=0}^{k-1}\binom{k-1}{i}/2^k \\
         & = k/2 \enspace .
\end{align*}
This requires that we know enough to calculate that $\Pr\{X=i\} = \binom{k}{i}/2^k$, and that we know the binomial identities $i\binom{k}{i}=k\binom{k-1}{i}$ and $\sum_{i=0}^{k} \binom{k}{i} = 2^{k}$.

Using indicator variables and linearity of expectation makes things much easier.  For each $i\in\{1,\ldots,k\}$, define the indicator random variable
\[
    I_i = \begin{cases}
           1 & \text{if the $i$th coin toss is heads} \\
           0 & \text{otherwise.}
          \end{cases}
\]
Then 
\[ \E[I_i] = (1/2)1 + (1/2)0 = 1/2 \enspace . \]
Now, $X=\sum_{i=1}^k I_i$, so
\begin{align*}
   \E[X] & = \E\left[\sum_{i=1}^k I_i\right] \\
         & = \sum_{i=1}^k \E[I_i] \\
         & = \sum_{i=1}^k 1/2 \\
         & = k/2 \enspace .
\end{align*}
This is a bit more long-winded, but doesn't require that we know any magical identities or compute any non-trivial probabilities. Even better, it agrees with the intuition that we expect half the coins to turn up as heads precisely because each individual coin turns up as heads with a probability of $1/2$.

\section{The Model of Computation}
\seclabel{model}

In this book, we will analyze the theoretical running times of operations on the data structures we study.  To do this precisely, we need a mathematical model of computation.  For this, we use the \emph{#w#-bit word-RAM} \index{word-RAM} \index{RAM} model.  RAM stands for Random Access Machine. In this model, we have access to a random access memory consisting of \emph{cells}, each of which stores a #w#-bit \emph{word}.  \index{word} This implies that a memory cell can represent, for example, any integer in the set $\{0,\ldots,2^{#w#}-1\}$.

In the word-RAM model, basic operations on words take constant time.  This includes arithmetic operations (#+#, #-#, #*#, #/#, #%#), comparisons ($<$, $>$, $=$, $\le$, $\ge$), and bitwise boolean operations (bitwise-AND, OR, and exclusive-OR).

Any cell can be read or written in constant time.  A computer's memory is managed by a memory management system from which we can allocate or deallocate a block of memory of any size we would like. Allocating a block of memory of size $k$ takes $O(k)$ time and returns a reference (a pointer) to the newly-allocated memory block.  This reference is small enough to be represented by a single word.

The word-size #w# is a very important parameter of this model.  The only assumption we will make about #w# is the lower-bound $#w# \ge \log #n#$, where #n# is the number of elements stored in any of our data structures.  This is a fairly modest assumption, since otherwise a word is not even big enough to count the number of elements stored in the data structure.

Space is measured in words, so that when we talk about the amount of space used by a data structure, we are referring to the number of words of memory used by the structure.  All of our data structures store values of a generic type #T#, and we assume an element of type #T# occupies one word of memory.  \javaonly{(In reality, we are storing references to objects of type #T#, and these references occupy only one word of memory.)}

\javaonly{The #w#-bit word-RAM model is a fairly close match for the (32-bit) Java Virtual Machine (JVM) when $#w#=32$.}\cpponly{The #w#-bit word-RAM model is a fairly close match for modern desktop computers when $#w#=32$ or $#w#=64$.} The data structures presented in this book don't use any special tricks that are not implementable \javaonly{on the JVM and most other architectures.}\cpponly{in C++ on most architectures.} 
\section{Correctness, Time Complexity, and Space Complexity}

When studying the performance of a data structure, there are three things that matter most:

\begin{description}
  \item[Correctness:] The data structure should correctly implement
    its interface.
    \index{correctness}
  \item[Time complexity:] The running times of operations on the data
    structure should be as small as possible.
    \index{time complexity}
    \index{complexity!time}
  \item[Space complexity:] The data structure should use as little memory
    as possible.
    \index{space complexity}
    \index{complexity!space}
\end{description}

%Sometimes these three requirements are in conflict with each other. For
%example, it may be possible to have a faster data structure by using
%more space.  It may be possible to have a data structure that is faster,
%or uses less space, if the data structure is allowed to make (hopefully
%occasional) mistakes.

In this introductory text, we will take correctness as a given;  we won't consider data structures that give incorrect answers to queries or don't perform updates properly.  We will, however, see data structures that make an extra effort to keep space usage to a minimum.  This won't usually affect the (asymptotic) running times of operations, but can make the data structures a little slower in practice.

When studying running times in the context of data structures we tend to come across three different kinds of running time guarantees:

\begin{description}
\item[Worst-case running times:] 
  \index{running time}
  \index{running time!worst-case}
  \index{worst-case running time}
  These are the strongest kind of running time guarantees.  If a data structure operation has a worst-case running time of $f(#n#)$, then one of these operations \emph{never} takes longer than $f(#n#)$ time.
\item[Amortized running times:]
  \index{running time!amortized}
  \index{amortized running time}
  If we say that the amortized running time of an operation in a data structure is $f(#n#)$, then this means that the cost of a typical operation is at most $f(#n#)$.  More precisely, if a data structure has an amortized running time of $f(#n#)$, then a sequence of $m$ operations takes at most $mf(#n#)$ time.  Some individual operations may take more than $f(#n#)$ time but the average, over the entire sequence of operations, is at most $f(#n#)$.
\item[Expected running times:] 
  \index{running time!expected}
  \index{expected running time}
  If we say that the expected running time of an operation on a data structure is $f(#n#)$, this means that the actual running time is a random variable (see \secref{randomization}) and the expected value of this random variable is at most $f(#n#)$.  The randomization here is with respect to random choices made by the data structure.
\end{description}

To understand the difference between worst-case, amortized, and expected running times, it helps to consider a financial example.  Consider the cost of buying a house: \paragraph{Worst-case versus amortized cost:} \index{amortized cost} Suppose that a home costs \$120\,000.  In order to buy this home, we might get a 120 month (10 year) mortgage with monthly payments of \$1\,200 per month.  In this case, the worst-case monthly cost of paying this mortgage is \$1\,200 per month.

If we have enough cash on hand, we might choose to buy the house outright, with one payment of \$120\,000.  In this case, over a period of 10 years, the amortized monthly cost of buying this house is \[ \$120\,000 / 120\text{ months} = \$1\,000\text{ per month} \enspace .  \] This is much less than the \$1\,200 per month we would have to pay if we took out a mortgage.

\paragraph{Worst-case versus expected cost:}
\index{expected cost} Next, consider the issue of fire insurance on our \$120\,000 home.  By studying hundreds of thousands of cases, insurance companies have determined that the expected amount of fire damage caused to a home like ours is \$10 per month.  This is a very small number, since most homes never have fires, a few homes may have some small fires that cause a bit of smoke damage, and a tiny number of homes burn right to their foundations.  Based on this information, the insurance company charges \$15 per month for fire insurance.

Now it's decision time. Should we pay the \$15 worst-case monthly cost for fire insurance, or should we gamble and self-insure at an expected cost of \$10 per month?  Clearly, the \$10 per month costs less \emph{in expectation}, but we have to be able to accept the possibility that the \emph{actual cost} may be much higher.  In the unlikely event that the entire house burns down, the actual cost will be \$120\,000.

These financial examples also offer insight into why we sometimes settle for an amortized or expected running time over a worst-case running time.  It is often possible to get a lower expected or amortized running time than a worst-case running time. At the very least, it is very often possible to get a much simpler data structure if one is willing to settle for amortized or expected running times.

\section{Code Samples}

\pcodeonly{

The code samples in this book are written in pseudocode.  \index{pseudocode} These should be easy enough to read for anyone who has any programming experience in any of the most common programming languages of the last 40 years.  To get an idea of what the pseudocode in this book looks like, here is a function that computes the average of an array, #a#: \pcodeimport{ods/Algorithms.average(a)} As this code illustrates, assigning to a variable is done using the $\gets$ notation.
% WARNING: graphic typesetting of assignment operator
We use the convention that the length of an array, #a#, is denoted by #len(a)# and array indices start at zero, so that #range(len(a))# are the valid indices for #a#.  To shorten code, and sometimes make it easier to read, our pseudocode allows for (sub)array assignments.
The following two functions are equivalent:
\pcodeimport{ods/Algorithms.left_shift_a(a).left_shift_b(a)}
The following code sets all the values in an array to zero:
\pcodeimport{ods/Algorithms.zero(a)}
When analyzing the running time of code like this, we have to be careful; statements like 
    #a[0:len(a)] = 1#
or
   #a[1:len(a)] = a[0:len(a)-1]#
do not run in constant time. They run in $O(#len(a)#)$ time.

We take similar shortcuts with variable assignments, so that the code #x,y=0,1# sets #x# to zero and #y# to 1 and the code #x,y = y,x# swaps the values of the variables #x# and #y#.  \index{swap}

Our pseudocode uses a few operators that may be unfamiliar.  As is standard in mathematics, (normal) division is denoted by the $/$ operator.  In many cases, we want to do integer division instead, in which case we use the $#//#$ operator, so that $#a//b# = \lfloor a/b\rfloor$ is the integer part of $a/b$.  So, for example, $3/2=1.5$ but $#3//2# = 1$.  \index{integer division} \index{div operator} Occasionally, we also use the $\bmod$ operator to obtain the remainder from integer division, but this will be defined when it is used.  \index{mod operator} \index{div operator} Later in the book, we may use some bitwise operators including left-shift (#<<#), right shift (#>>#), bitwise and (#&#), and bitwise exclusive-or (#^#).  \index{left shift} \index{#<<#|see {left shift}} \index{right shift} \index{#>>#|see {right shift}} \index{bitwise and} \index{#&#|see {bitwise and}} \index{#^#|see {bitwise exclusive-or}}

The pseudocode samples in this book are machine translations of Python code that can be downloaded from the book's website.\footnote{ \url{http://opendatastructures.org}}  If you ever encounter an ambiguity in the pseudocode that you can't resolve yourself, then you can always refer to the corresponding Python code.  If you don't read Python, the code is also available in Java and C++.  If you can't decipher the pseudocode, or read Python, C++, or Java, then you may not be ready for this book.  }

\notpcode{
The code samples in this book are written in the \lang\ programming language.  However, to make the book accessible to readers not familiar with all of \lang's constructs and keywords, the code samples have been simplified.  For example, a reader won't find any of the keywords #public#, #protected#, #private#, or #static#.  A reader also won't find much discussion about class hierarchies.  Which interfaces a particular class implements or which class it extends, if relevant to the discussion, should be clear from the accompanying text.

These conventions should make the code samples understandable by anyone with a background in any of the languages from the ALGOL tradition, including B, C, C++, C\#, Objective-C, D, Java, JavaScript, and so on.  Readers who want the full details of all implementations are encouraged to look at the \lang\ source code that accompanies this book.

This book mixes mathematical analyses of running times with \lang\ source code for the algorithms being analyzed.  This means that some equations contain variables also found in the source code.  These variables are typeset consistently, both within the source code and within equations.  The most common such variable is the variable #n# \index{n@#n#} that, without exception, always refers to the number of items currently stored in the data structure. 
}

\section{List of Data Structures}

Tables~\ref{tab:summary-i} and \ref{tab:summary-ii} summarize the performance of data structures in this book that implement each of the interfaces, #List#, #USet#, and #SSet#, described in \secref{interfaces}.  \Figref{dependencies} shows the dependencies between various chapters in this book.  \index{dependencies} A dashed arrow indicates only a weak dependency, in which only a small part of the chapter depends on a previous chapter or only the main results of the previous chapter.

\begin{table}
\vspace{56pt}
\begin{center}
\resizebox{.98\textwidth}{!}{
\begin{threeparttable}
\begin{tabular}{|l|l|l|l|} \hline
\multicolumn{4}{|c|}{#List# implementations} \\ \hline
 & #get(i)#/#set(i,x)# & #add(i,x)#/#remove(i)# & \\ \hline
#ArrayStack# & $O(1)$ & $O(1+#n#-#i#)$\tnote{A} & \sref{arraystack} \\
#ArrayDeque# & $O(1)$ & $O(1+\min\{#i#,#n#-#i#\})$\tnote{A} & \sref{arraydeque} \\
#DualArrayDeque# & $O(1)$ & $O(1+\min\{#i#,#n#-#i#\})$\tnote{A} & \sref{dualarraydeque}\\
#RootishArrayStack# & $O(1)$ & $O(1+#n#-#i#)$\tnote{A}  & \sref{rootisharraystack} \\
#DLList# & $O(1+\min\{#i#,#n#-#i#\})$ & $O(1+\min\{#i#,#n#-#i#\})$  & \sref{dllist} \\
#SEList# & $O(1+\min\{#i#,#n#-#i#\}/#b#)$ & $O(#b#+\min\{#i#,#n#-#i#\}/#b#)$\tnote{A}  & \sref{selist} \\
#SkiplistList# & $O(\log #n#)$\tnote{E} & $O(\log #n#)$\tnote{E}  & \sref{skiplistlist} \\ \hline
\multicolumn{4}{c}{} \\[2ex] \hline
\multicolumn{4}{|c|}{#USet# implementations} \\ \hline
 & #find(x)# & #add(x)#/#remove(x)# & \\ \hline
#ChainedHashTable# & $O(1)$\tnote{E} & $O(1)$\tnote{A,E} & \sref{hashtable} \\ 
#LinearHashTable# & $O(1)$\tnote{E} & $O(1)$\tnote{A,E} & \sref{linearhashtable} \\ \hline
\end{tabular}
\begin{tablenotes}
\item[A]{Denotes an \emph{amortized} running time.}
\item[E]{Denotes an \emph{expected} running time.}
\end{tablenotes}
\end{threeparttable}}
\end{center}
\caption[Summary of List and USet implementations.]{Summary of #List# and #USet# implementations.}
\tablabel{summary-i}
\end{table}

\begin{table}
\begin{center}
\begin{threeparttable}
\begin{tabular}{|l|l|l|l|} \hline
\multicolumn{4}{|c|}{#SSet# implementations} \\ \hline
 & #find(x)# & #add(x)#/#remove(x)# & \\ \hline
#SkiplistSSet# & $O(\log #n#)$\tnote{E} & $O(\log #n#)$\tnote{E} & \sref{skiplistset} \\ 
#Treap# & $O(\log #n#)$\tnote{E} & $O(\log #n#)$\tnote{E} & \sref{treap} \\ 
#ScapegoatTree# & $O(\log #n#)$ & $O(\log #n#)$\tnote{A} & \sref{scapegoattree} \\
#RedBlackTree# & $O(\log #n#)$ & $O(\log #n#)$ & \sref{redblacktree} \\ 
#BinaryTrie#\tnote{I} & $O(#w#)$ & $O(#w#)$ & \sref{binarytrie} \\ 
#XFastTrie#\tnote{I} & $O(\log #w#)$\tnote{A,E} & $O(#w#)$\tnote{A,E} & \sref{xfast} \\ 
#YFastTrie#\tnote{I} & $O(\log #w#)$\tnote{A,E} & $O(\log #w#)$\tnote{A,E} & \sref{yfast} \\ 
\javaonly{#BTree# & $O(\log #n#)$ & $O(B+\log #n#)$\tnote{A} & \sref{btree} \\ 
#BTree#\tnote{X} & $O(\log_B #n#)$ & $O(\log_B #n#)$ & \sref{btree} \\ } \hline
\multicolumn{4}{c}{} \\[2ex] \hline
\multicolumn{4}{|c|}{(Priority) #Queue# implementations} \\ \hline
 & #findMin()# & #add(x)#/#remove()# & \\ \hline
#BinaryHeap# & $O(1)$ & $O(\log #n#)$\tnote{A} & \sref{binaryheap} \\ 
#MeldableHeap# & $O(1)$ & $O(\log #n#)$\tnote{E} & \sref{meldableheap} \\ \hline
\end{tabular}
\begin{tablenotes}
\item[I]{This structure can only store #w#-bit integer data.}
\javaonly{\item[X]{This denotes the running time in the external-memory model; see \chapref{btree}.}}
\end{tablenotes}
%\renewcommand{\thefootnote}{\arabic{footnote}}
\end{threeparttable}
\end{center}
\caption[Summary of SSet and priority Queue implementations.]{Summary of #SSet# and priority #Queue# implementations.}
\tablabel{summary-ii}
\end{table}

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/dependencies}
  \end{center}
  \caption{The dependencies between chapters in this book.}
  \figlabel{dependencies}
\end{figure}

\section{Discussion and Exercises}

The #List#, #USet#, and #SSet# interfaces described in \secref{interfaces} are influenced by the Java Collections Framework \cite{oracle_collections}.  \index{Java Collections Framework} These are essentially simplified versions of the #List#, #Set#, #Map#, #SortedSet#, and #SortedMap# interfaces found in the Java Collections Framework.  \javaonly{The accompanying source code includes wrapper classes for making #USet# and #SSet# implementations into #Set#, #Map#, #SortedSet#, and #SortedMap# implementations.}

For a superb (and free) treatment of the mathematics discussed in this chapter, including asymptotic notation, logarithms, factorials, Stirling's approximation, basic probability, and lots more, see the textbook by Leyman, Leighton, and Meyer \cite{llm11}.  For a gentle calculus text that includes formal definitions of exponentials and logarithms, see the (freely available) classic text by Thompson \cite{t14}.

For more information on basic probability, especially as it relates to computer science, see the textbook by Ross \cite{r01}.  Another good reference, which covers both asymptotic notation and probability, is the textbook by Graham, Knuth, and Patashnik \cite{gkp94}.

\javaonly{Readers wanting to brush up on their Java programming can find many Java tutorials online \cite{oracle_tutorials}.}

\begin{exc}
  This exercise is designed to help familiarize the reader with choosing the right data structure for the right problem.  If implemented, the parts of this exercise should be done by making use of an implementation of the relevant interface (#Stack#, #Queue#, #Deque#, #USet#, or #SSet#) provided by the \javaonly{Java Collections Framework}\cpponly{C++ Standard Template Library}.

  Solve the following problems by reading a text file one line at a time and performing operations on each line in the appropriate data structure(s).  Your implementations should be fast enough that even files containing a million lines can be processed in a few seconds.
  \begin{enumerate}
    \item Read the input one line at a time and then write the lines out in reverse order, so that the last input line is printed first, then the second last input line, and so on.

    \item  Read the first 50 lines of input and then write them out in reverse order. Read the next 50 lines and then write them out in reverse order. Do this until there are no more lines left to read, at which point any remaining lines should be output in reverse order.

      In other words, your output will start with the 50th line, then the 49th, then the 48th, and so on down to the first line. This will be followed by the 100th line, followed by the 99th, and so on down to the 51st line. And so on.  Your code should never have to store more than 50 lines at any given time.

    \item Read the input one line at a time.  At any point after reading the first 42 lines, if some line is blank (i.e., a string of length 0), then output the line that occured 42 lines prior to that one. For example, if Line 242 is blank, then your program should output line 200. This program should be implemented so that it never stores more than 43 lines of the input at any given time.

    \item Read the input one line at a time and write each line to the output if it is not a duplicate of some previous input line. Take special care so that a file with a lot of duplicate lines does not use more memory than what is required for the number of unique lines.

    \item Read the input one line at a time and write each line to the output only if you have already read this line before. (The end result is that you remove the first occurrence of each line.) Take special care so that a file with a lot of duplicate lines does not use more memory than what is required for the number of unique lines.

    \item Read the entire input one line at a time. Then output all lines sorted by length, with the shortest lines first. In the case where two lines have the same length, resolve their order using the usual ``sorted order.''  Duplicate lines should be printed only once.

    \item Do the same as the previous question except that duplicate lines should be printed the same number of times that they appear in the input.

    \item Read the entire input one line at a time and then output the even numbered lines (starting with the first line, line 0) followed by the odd-numbered lines.

    \item Read the entire input one line at a time and randomly permute the lines before outputting them.  To be clear: You should not modify the contents of any line. Instead, the same collection of lines should be printed, but in a random order.
  \end{enumerate}
\end{exc}

\begin{exc}
  \index{Dyck word}
  A \emph{Dyck word} is a sequence of +1's and -1's with the property that the sum of any prefix of the sequence is never negative.  For example, $+1,-1,+1,-1$ is a Dyck word, but $+1,-1,-1,+1$ is not a Dyck word since the prefix $+1-1-1<0$.  Describe any relationship between Dyck words and #Stack# #push(x)# and #pop()# operations.
\end{exc}

\begin{exc}
  \index{matched string}
  \index{string!matched}
  A \emph{matched string} is a sequence of \{, \}, (, ), [, and ] characters that are properly matched.  For example, ``\{\{()[]\}\}'' is a matched string, but this ``\{\{()]\}'' is not, since the second \{ is matched with a ].  Show how to use a stack so that, given a string of length #n#, you can determine if it is a matched string in $O(#n#)$ time.
\end{exc}

\begin{exc}
  Suppose you have a #Stack#, #s#, that supports only the #push(x)# and #pop()# operations. Show how, using only a FIFO #Queue#, #q#, you can reverse the order of all elements in #s#.
\end{exc}

\begin{exc}
  \index{Bag@#Bag#}
  Using a #USet#, implement a #Bag#.  A #Bag# is like a #USet#---it supports the #add(x)#, #remove(x)# and #find(x)# methods---but it allows duplicate elements to be stored.  The #find(x)# operation in a #Bag# returns some element (if any) that is equal to #x#.  In addition, a #Bag# supports the #findAll(x)# operation that returns a list of all elements in the #Bag# that are equal to #x#.
\end{exc}

\begin{exc}
  From scratch, write and test implementations of the #List#, #USet# and #SSet# interfaces.  These do not have to be efficient.  They can be used later to test the correctness and performance of more efficient implementations.  (The easiest way to do this is to store the elements in an array.)
\end{exc}

\begin{exc}
  Work to improve the performance of your implementations from the previous question using any tricks you can think of.  Experiment and think about how you could improve the performance of #add(i,x)# and #remove(i)# in your #List# implementation.  Think about how you could improve the performance of the #find(x)# operation in your #USet# and #SSet# implementations.  This exercise is designed to give you a feel for  how difficult it can be to obtain efficient implementations of these interfaces.
\end{exc}




