\chapter{Introduction}
\pagenumbering{arabic}

世界中のコンピュータサイエンスカリキュラムにデータ構造とアルゴリズムに関するコースが含まれている。データ構造は重要だ。生活の質を上げ、日常生活を効率的にしてくれる。数百万ドル、数十億ドルの企業にはデータ構造を中心に作られたものも多い。

どういうものなのだろう？深く考えるのをやめると、データ構造と普段から接していることに気づく。
\begin{itemize}
	\item ファイルを開く：ファイルシステム\index{file system}のデータ構造を使って、ファイルをディスク上に配置し、検索できる。これは簡単ではない。ディスクは数億のブロックを含む。ファイルの内容はそのどこかに保存されるのだ。
	\item 連絡先を検索する：ダイヤル途中の部分的な情報にもとづき、連絡先リスト\index{contact list}から電話番号を見つけるためにデータ構造が使われる。これは簡単ではない。連絡先リストに含まれる情報はとても多いかもしれない。これまで電話や電子メールで連絡したことのある全ての人を想像してみてほしい。また、電話のプロセッサはあまり高性能でなく、メモリも潤沢でない。
	\item SNSにログインする：\index{social network}ネットワークサーバーは、ログイン情報からアカウント情報を検索する。これは簡単ではない。人気のソーシャルネットワークには何億人ものアクティブユーザーがいる。
	\item Webページを検索する：\index{web search}検索エンジンは検索語を含むWebページを見つけるためにデータ構造を使う。これは簡単ではない。インターネットには85億以上のWebページがあり、個々のページには検索されるかもしれない単語が多く含まれている。
	\item 緊急サービス（9-1-1）に電話する：\index{emergency services}\index{9-1-1}緊急サービスネットワークはパトカー、救急車、消防車が速やかに現場に到着できるよう、電話番号と住所を対応づけるデータ構造を使う。これは重要だ。電話をかけた人は正確な住所を伝えられないかもしれず、遅れが生死を別つ可能性があるためである。
\end{itemize}

\section{効率の必要性}

次節では多くのデータ構造がサポートする操作を見ていく。ちょっとしたプログラミング経験があれば、これらを正しく実装することは難しくない。データを配列または線形リストに格納し、配列または線形リストの全要素を見てまわり、要素を追加したり削除したりすればよいのだ。

こういう実装は簡単だが、あまり効率的ではない。さて、このことを考える価値はあるだろうか？コンピュータはどんどん高速化している。自明な実装で十分だろう。確認のためにざっくりとした計算をしてみよう。

\paragraph{操作の数：}
まあまあの大きさのデータセット、例えば100万（$10^6$）のアイテムを持つアプリケーションがあるとする。少なくとも一度は各アイテムを参照すると仮定してよいことが多いだろう。つまり少なくとも100万（$10^6$）回はこのデータで検索をしたいわけだ。この$10^6$回の検索それぞれが$10^6$個のアイテムをすべて確認すると、合計$10^6\times 10^6=10^{12}$（1000億）回の確認処理が必要だ。

\paragraph{プロセッサの速度：}
本書の執筆の時点で、かなり高速なデスクトップコンピュータでも毎秒10億（$10^9$）の操作は実行できない。\footnote{コンピュータの速度はせいぜい数ギガヘルツ（数十億回/秒）であり、各操作にふつう数サイクルが必要だ。}よってこのアプリケーションの完了には少なくとも$10^{12}/10^9=1000$秒、すなわち約16分40秒かかる。コンピューターの時間では16分は非常に長いが、人は気にしないかもしれない。（例えば、プログラマがコーヒーブレイクに向かうならそれで構わないだろう。）

\paragraph{大きなデータセット：}
Google\index{Google|}を考えてみよう。Googleでは85億ものWebページからの検索を扱う。先ほどの計算では、このデータに対するクエリは少なくとも8.5秒かかる。だがこれが事実ではないことはわかるだろう。Web検索には8.5秒もかからないし、あるページがインデックスに含まれているかよりさらに複雑なクエリを実行する。執筆時点でGoogleは1秒間に約$4,500$クエリを受けつける。つまり少なくとも$4,500 \times 8.5 = 38,250$ものサーバーが必要なのだ。

\paragraph{解決策：}
以上の例から、アイテム数$\mathtt{n}$と実行される操作数$m$が共に大きくなると、データ構造の自明な実装はスケールしないことがわかる。今の例で、（機械命令数で数えた）時間はおよそ$#n#\times m$だ。

解決策はもちろん、データ構造内のデータを注意深く格納して、各操作がすべてのデータを見て回る必要がないようにすることだ。最初はムリなように思えるが、データ構造に格納されているデータの数とは関係なく、平均して2つのデータだけを参照すればすむデータ構造をのちに紹介する。 1秒あたり10億命令を実行できるとして、10億個のデータ（兆、京、垓であっても）を含むデータ構造を検索するのにわずか$0.000000002$秒しかかからないのだ。

データ構造内のデータを整列することで、データ数に対して参照されるデータ数が非常にゆっくり増加するデータ構造も後で紹介する。例えば10億個のアイテムを整列しておけば、最大60個のアイテムを参照することで各操作を実行できる。毎秒10億命令実行できるコンピュータでは、これらの操作にそれぞれ$0.00000006$秒かかる。

この章の残りの部分では、この本の残りの部分で使用される主な概念の一部を簡単に解説する。\secref{interface}は本書で説明する全データ構造で実装されるインターフェースを説明するので読む必要がある。残りの節では、以下のものを解説する。
\begin{itemize}
\item 指数・対数・階乗関数や漸近(ビッグオー)記法、確率、ランダム化などの数学の復習
\item 計算モデル
\item 正しさと実行時間、メモリ使用量
\item 残りの章の概要
\item サンプルコードと組版の規則
\end{itemize}
これらの背景知識があってもなくても、いったん気軽に飛ばし必要に応じて戻り読みしてもよい。

\section{インターフェイス}
\seclabel{インターフェイス}
データ構造について議論するときは、データ構造のインターフェースと実装の違いを理解することが重要だ。インターフェースはデータ構造が何をするかを、実装はデータ構造がどうやるかを示す。

\emph{インターフェース}\index{interface}\index{abstract data type|see{interface}}（\emph{抽象データ型}と呼ばれることもある）は、データ構造がサポートする操作一式とその意味を定義する。インターフェースを見ても操作がどう実装されているかはわからない。サポートする操作の一覧とその引数、返り値の特徴だけを教えてくれる。

一方でデータ構造の\emph{実装}には、データ構造の内部表現と、操作を実装するアルゴリズムの定義が含まれる。そのため、あるインターフェースに対する複数の実装が考えられる。例えば、\chapref{arrays}では配列を、\chapref{linkedlists}ではポインタを使って#List#インターフェースを実装する。それぞれ同じ#List#インターフェースを実装しているが、実装方法は異なるのだ。

\subsection{#Queue#、#Stack#、#Deque#インターフェース}

#Queue#インターフェースは要素を追加したり、次の要素を削除したりできる要素の集まりを表す。より正確には、#Queue#インターフェースがサポートする操作は以下のものだ。

\begin{itemize}
  \item #add(x)#：値#x#を#Queue#に追加する。
  \item #remove()#：（以前に追加された）次の値#y#を#Queue#から削除し、#y#を返す。
\end{itemize}

#remove()#操作の引数はないことに注意する。#Queue#は\emph{取り出し規則}に従って削除する要素を決める。取り出し規則は色々と考えられるが、主なものとしてはFIFOや優先度、LIFOなどがある。

\figref{queue}に示す\emph{FIFO（first-in-first-out、先入れ先出し）#Queue#}\index{FIFO queue} \index{queue!FIFO}は、追加したのと同じ順番で要素を削除する。これは食料品店のレジで並ぶ列と同じようなものだ。これは最も一般的な#Queue#なので、修飾子のFIFOは省略されることが多い。他の教科書ではFIFO #Queue#の#add(x)#、#remove()#は、それぞれ#enqueue(x)#、#dequeue()#と呼ばれていることも多い。

\begin{figure}
  \centering{\includegraphics[width=\ScaleIfNeeded]{figs/queue}}
  \caption[FIFOキュー]{FIFO #Queue#.}
  \figlabel{queue}
\end{figure}

\fig{prioqueue}に示す優先度付き#Queue#は、
\index{priority queue}%
\index{priority queue|seealso{heap}}%
\index{queue!priority}%
#Queue#から最小の要素を削除します。同点要素が複数あるときは、そのうちのいずれかを適当に選ぶ。これは病院の救急室で重症患者を優先的に治療することに似ている。患者が到着したら、症状を見積もってから待合室で待機していてもらう。医師の手が空くと、最も重篤な患者から治療する。優先度付き#Queue#における#remove()#操作を、他の教科書ではよく#deleteMin()#などと呼んでいる。

\begin{figure}
  \centering{\includegraphics[width=\ScaleIfNeeded]{figs/prioqueue}}
  \caption[優先度付きキュー]{A priority #Queue#.}
  \figlabel{prioqueue}
\end{figure}

よく使う取り出し規則は、図1.3に示すLIFO（last-in-first-out、後入れ先出し）
\index{LIFO queue}%
\index{LIFO queue|seealso{stack}}%
\index{queue!LIFO}%
\index{stack}%
だ。 \emph{LIFOキュー}では、最後に追加された要素が次に削除される。これはプレートを積むように視覚化するとよい。プレートはスタックの上に積まれ、上から順に持って行かれる。この構造はとてもよく見かけるので#Stack#という名前が付いている。#Stack#について話すとき、#add(x)#、#remove()#のことを、#push(x)#、#pop()#と呼ぶ。こうすればLIFOとFIFOの取り出し規則を区別できる。

\begin{figure}
  \centering{\includegraphics[width=\ScaleIfNeeded]{figs/stack}}
  \caption[スタック]{スタック}
  \figlabel{stack}
\end{figure}

#Deque#（双方向キュー）はFIFOキューとLIFOキュー（スタック）の一般化だ。 #Deque#は前と後ろのある要素の列を表す。列の前または後ろに要素を追加できる。 #Deque#操作の名前はわかりやすく、#addFirst(x)#、#removeFirst()#、#addLast(x)#、 #removeLast()#だ。スタックは#addFirst()#と#removeFirst()#だけを使って実装できる。一方FIFOキューには#addLast(x)#と#removeFirst()#を使えばよい。

\subsection{#List#インターフェース：線形シーケンス}

この本では、FIFO #Queue#や#Stack#、#Deque#のインターフェースについての話はほとんどしない。なぜなら、これらは#List#インターフェースにまとめられるためだ。図1.4に示す#List#\index{List@#List#}は、値の列$#x#_0,\ldots,#x#_{#n#-1}$を表現する。
#List#インターフェイスは以下の操作を含む。

\begin{enumerate}
  \item #size()#: リストの長さ#n#を返す。
  \item #get(i)#: $#x#_{#i#}$の値を返す。
  \item #set(i,x)#: $#x#_{#i#}$の値を#x#にする。
  \item #add(i,x)#: #x#を#i#番目として追加し、$#x#_{#i#},\ldots,#x#_{#n#-1}$をずらす。\\
    すなわち、$j\in\{#n#-1,\ldots,#i#\}$について$#x#_{j+1}=#x#_j$とし、#n#をひとつ増やし、$#x#_i=#x#$とする。
  \item #remove(i)#: $#x#_{#i#}$を削除し、$#x#_{#i+1#},\ldots,#x#_{#n#-1}$をずらす。\\ 
    すなわち、$j\in\{#i#,\ldots,#n#-2\}$について$#x#_{j}=#x#_{j+1}$とし、#n#をひとつ減らす。
\end{enumerate}

これらの操作を使って、#Deque#インターフェースは簡単に実装できる。

\begin{eqnarray*}
  #addFirst(x)# &\Rightarrow& #add(0,x)# \\
  #removeFirst()# &\Rightarrow& #remove(0)#  \\
  #addLast(x)# &\Rightarrow& #add(size(),x)# \\
  #removeLast()# &\Rightarrow& #remove(size()-1)#
\end{eqnarray*}

\begin{figure}
  \centering{\includegraphics[width=\ScaleIfNeeded]{figs/list}}
  \caption[A List]{#List#は$0,1,2,\ldots,#n#-1$で添え字づけられた列を表現する。この#List#で#get(2)#を実行すると値$c$が返ってくる。}
  \figlabel{list}
\end{figure}


この後の章では#Stack#、#Deque#、FIFO #Queue#のインターフェースについての話はほぼ出てこない。しかし、#Stack#と#Deque#は、#List#インターフェースを実装するデータ構造として出てくることがある。その場合、#Stack#や#Deque#のインターフェースは非常に効率良く実装できる。たとえば#ArrayDeque#クラスは#List#インターフェースの実装だ。これはすべての#Deque#操作をひとつだけの定数時間操作で実装できる。

\subsection{#USet#インターフェース：順序付けられていない要素の集まり}

#USet#\index{USet@#USet#}インターフェースは重複がなく、順序付けられていない要素の集まりを表現する。これは数学における\emph{集合}を模したものだ。#USet#には、#n#個の\emph{互いに相異なる}要素が含まれる。つまり、同じ要素が複数入っていることはない。また、要素の並び順は決まっていない。#USet#は以下の操作をサポートする。

\begin{enumerate}
\item #size()#：集合の要素数#n#を返す。
\item #add(x)#：要素#x#が集合に入っていなければ集合に追加する。\\
$#x# = #y#$を満たす集合の要素#y#が存在しないなら、集合に#x#を加える。#x#が集合に追加されたら#true#を返し、そうでなければ#false#を返す。
\item #remove(x)#：集合から#x#を削除する。\\
$#x# = #y#$を満たす集合の要素#y#を探し、集合から取り除く。そのような要素が見つかれば#y#を、見つからなければ#null#を返す。
\item #find（x）#：集合に#x#が入っていればそれを見つける。\\
$#x# = #y#$を満たす集合の要素#y#を見つける。そのような要素が見つかれば#y#を、見つからなければ#null#を返す。
\end{enumerate}

今述べた定義で、探したい#x#と、見つかるかもしれない集合の要素#y#の区別を小難しく感じるかもしれない。これを区別する理由は、#x#と#y#は等しいと判定される別のオブジェクトかもしれないためだ。こうすると、キーを値に対応づける\emph{辞書}(\emph{マップ})を作るのに便利なのだ。
\index{dictionary}%
\index{map}%

辞書(マップ)を作るために、まず#Pair#\index{pair}という複合オブジェクトを作る。#Pair#には\emph{キー}と\emph{値}からなる。 2つの#Pair#は、キーが等しいければ等しいとみなされる。#Pair#である$(#k#,#v#)$を#USet#に入れてから、$#x#=(#k#,#null#)$として#find(x)#を呼び出すと、$#y#=(#k#,#v#)$が返ってくる。すなわち、キー#k#だけから値#v#を復元できるのだ。

\subsection{#SSet#インターフェース：ソートされた要素の集まり}
\seclabel{sset}

\index{SSet@#SSet#}%
#SSet#インターフェースは順序づけされた要素の集まりを表現する。#SSet#は全順序な要素を格納するので、任意の2つの要素#x#と#y#は比較可能である。サンプルコードでは、以下のように定義される#compare(x, y)#メソッドで比較を行うものとする。

\[
    #compare(x,y)#
      \begin{cases}
        {}<0 & \text{if $#x#<#y#$} \\
        {}>0 & \text{if $#x#>#y#$} \\
        {}=0 & \text{if $#x#=#y#$}
      \end{cases}
\]
\index{compare@#compare(x,y)#}%

#SSET#は#USet#と全く同じセマンティクスを持つ操作#size()#、#add(x)#、#remove(x)#をサポートする。#USet#と#SSet#の違いは#find(x)#だ。

\begin{enumerate}
\setcounter{enumi}{3}
\item #find(x)#: 順序づけられた集合から#x#の位置を特定する。\\
   $#y# \ge #x#$を満たす最小の要素#y#を探す。
   このような#y#が存在すれば返し、そうでないなら#null#を返す。
\end{enumerate}

この#find(x)#は、\emph{後継探索(XXX:訳語)}\index{successor search}と呼ばれることがある。#x#に等しい要素がなくても意味のある結果を返す点で#USet.find(x)#とは異なる。

#USet#、#SSet#における#find(x)#の区別は重要なのだが気づいていない人もいる。 #SSet#は追加の仕事をしてくれるぶん、実装が複雑で実行時間が長くなりがちだ。例えば、この本で述べる#SSet#の#find(x)#の実装では、要素数の対数だけの時間がかかる。一方、\chapref{hashing}の#ChainedHashTable#による#USet#の実装では、#find(x)#の実行時間の期待値は定数である。#SSet#の追加機能が本当に必要でないなら、#SSet#ではなく常に#USet#を使うべきだ。

\section{数学的背景}
この節では本書で使用する数学記法や基礎知識を復習する。例えば、対数やビッグオー記法、確率論などだ。内容はあっさりしたもので、丁寧な手解きはしない。背景知識が足りないと感じる読者のために、コンピュータサイエンスのための数学の優れた無料の教科書がある。必要に応じて適切な箇所を読み、練習問題を解いてみるとよいだろう。
\cite{llm11}.

\subsection{指数と対数}

\index{exponential}
$b^x$と書いて$b$の$x$乗を表す。$x$が正の整数なら、$b$にそれ自身を$x-1$回掛けた値になる。

\[
    b^x = \underbrace{b\times b\times \cdots \times b}_{x} \enspace .
\]

$ x $が負の整数なら、$b^x=1/b^{-x}$である。$x=0$なら、$b^x=1$である。
$b$が整数でないときも、やはり（後述する）指数関数$e^x$の観点から、べき乗を定義できる。指数関数もまた指数級数を使って定義されているが、こういうな話は微積分の教科書に任せることにする。

\index{logarithm}
この本では$\log_b k$と書いて\emph{$b$を底とする対数}を表す。これは次の式を満たす$x$として一意に決まる、

\[
    b^{x} = k  \enspace .
\]

この本に出てくる対数の底は2であることが多い。底が2の対数を\{二進対数}という。そのため、底になにも書かない$\log k$は$\log_2 k$の省略記法とする。
\index{binary logarithm}%
\index{logarithm!binary}%

対数の大雑把なイメージを持つ方法を紹介する。$\log_b k$とは$k$を何回$b$で割ると1より小さくなるかを表す数だと考えればよい。例を挙げよう。二分探索という手法を使うと、一回の比較処理のたびに、答えの候補の個数が半分になる。答えの候補が1つに絞られるまで、この処理を繰り返す。$n+1$個の答えの候補が最初にあるなら、二分検索に必要な比較の回数は$\lceil \log_2(n+1) \rceil$以下だ。

\index{natural logarithm}%
\index{logarithm!natural}%
この本で\emph{自然対数}という別の対数も何度か出てくる。$\ln k$と書いて$\log_e k$を表すことにする。ここで、$e$は次のように定義される\emph{オイラーの定数}だ。
\index{Euler's constant}%
\index{e@$e$ (Euler's constant)}%
\[
   e = \lim_{n\rightarrow\infty} \left(1+\frac{1}{n}\right)^n
   \approx  2.71828 \enspace .
\]

自然対数は頻繁に現れる。これは$e$がよく見かける次のような積分値であるためだ。
\[
    \int_{1}^{k} 1/x\,\mathrm{d}x  = \ln k \enspace .
\]

よく使う対数の操作は2つある。ひとつは指数部からの取り出し操作だ。
\[
    b^{\log_b k} = k
\]

もう一つは対数の底を取り替え操作だ。
\[
    \log_b k = \frac{\log_a k}{\log_a b} \enspace .
\]

これら2つの操作を使えば、例えば自然対数と二進対数を比較できる。
\[
   \ln k = \frac{\log k}{\log e} = \frac{\log k}{(\ln e)/(\ln 2)} =
    (\ln 2)(\log k) \approx 0.693147\log k \enspace .
\]

\subsection{階乗}
\seclabel{factorials}

\index{factorials}
この本で\emph{階乗関数}を使う箇所がいくつかある。$n$が非負整数のとき、$n!$（ 「$n$の階乗」と読む）は次のように定義される。
\[
   n! = 1\cdot2\cdot3\cdot\cdots\cdot n \enspace .
\]

$n!$は$n$要素の相異なる順列\index{permutation}の個数である。つまり$n$個の相異なる要素の並べ方の数として階乗は現れる。なお、$n=0$のときについて、$0!$は1と定義される。

\index{Stirling's Approximation}%
$n!$の大きさは、\emph{スターリングの近似}によって近似的に求められる。
\[
	n!
   = \sqrt{2\pi n}\left(\frac{n}{e}\right)^{n}e^{\alpha(n)} \enspace ,
\]
ここで$\alpha(n)$は次の条件を満たす。
\[
   \frac{1}{12n+1} <  \alpha(n) < \frac{1}{12n}  \enspace .
\]

スターリングの近似を使って$\ln(n!)$の近似値も計算できる。
\[
   \ln(n!) = n\ln n - n + \frac{1}{2}\ln(2\pi n) + \alpha(n)
\]

（スターリングの近似を証明する簡単な方法として、$\ln(n!)=\ln 1 + \ln 2  + \cdots + \ln n$を$\int_1^n \ln n\,\mathrm{d}n = n\ln n - n +1$で近似するというものがある。)

\index{binomial coefficients}%
\emph{二項係数}は階乗関数とつながりがある。$n$を非負整数、$k$を$\{0,\ldots,n\}$の要素とするとき、$\binom{n}{k}$は次のように定義される。

\[
   \binom{n}{k} = \frac{n!}{k!(n-k)!} \enspace .
\]
二項係数$\binom{n}{k}$は、大きさ$n$の集合における大きさ$k$の部分集合の個数である。すなわち、集合$\{1,\ldots,n\}$から相異なる$k$個の整数を取り出すときの場合の数である。

\subsection{漸近記法}

\index{asymptotic notation} \index{big-Oh notation} \index{O@$O$ notation}
データ構造を分析する際には操作の実行時間についての議論が有用だ。正確な実行時間はコンピュータごとに異なる。同じコンピュータで実行する場合ですらバラつくだろう。実行時間の話をしているときは、実際は操作に必要なコンピュータ命令数に注目する。単純なコードであっても、この量を正確に計算するのは難しいことがある。そのため実行時間を正確に解析するのではなく、いわゆる\emph{ビッグオー記法}を使う。$f(n)$を関数とするとき、$O(f(n))$は次のような関数の集合を表す。
\[
   O(f(n)) = \left\{
     \begin{array}{l}
       g(n):\mbox{there exists $c>0$, and $n_0$ such that} \\
             \quad\mbox{$g(n) \le c\cdot f(n)$ for all $n\ge n_0$}
     \end{array} \right\} \enspace .
\]
図形的に考える。$n$が十分に大きくなると$c\cdot f(n)$に上から抑えられる関数$g(n)$を集めたものがこの集合だ。

漸近表記を使えば関数を単純化できる。たとえば、$5n\log n + 8n - 200$の代わりに$O(n \log n)$と書ける。これは次のように証明できる。
\begin{align*} 
       5n\log n + 8n - 200
        & \le 5n\log n + 8n \\
        & \le 5n\log n + 8n\log n & \mbox{ for $n\ge 2$ (so that $\log n \ge 1$)}
            \\
        & \le 13n\log n  \enspace .
\end{align*}
$ c = 13 $および$ n_0 = 2 $とすれば、関数$ f(n)= 5n \log n + 8n-200 $が集合$ O(n \log n)$に含まれることがわかる。

漸近表記の便利な性質をいくつか挙げる。

まずは、任意の定数$c_1 < c_2$について以下が成り立つ。
\[ O(n^{c_1}) \subset O(n^{c_2}) \enspace\]

つづいて、任意の定数$ a、b、c> 0 $について以下が成り立つ。
\[ O(a) \subset O(\log n) \subset O(n^{b}) \subset O({c}^n) \enspace\]

これらの包含関係はそれぞれに正の値を掛けても保たれる。
たとえば$n$を掛けると次のようになります。

\[ O(n) \subset O(n\log n) \subset O(n^{1+b}) \subset O(n{c}^n) \enspace \]

これは有名な記号の濫用なのだが、$f_1(n) = O(f(n))$ と書いて$f_1(n) \in O(f(n))$であることを表す。また、「この操作の実行時間は$O(f(n))$に\emph{含まれる}」ことを単に「この操作の実行時間は$O(f(n))$だ」と言う。これらの短い言い方は語感を整え、漸近記法を連続する等式の中で使いやすくするのに役立つ。

この書き方の、奇妙な例を挙げる。
\[
  T(n) = 2\log n + O(1)  \enspace
\]

これは正確に書くとこうなる。
\[
  T(n) \le 2\log n + [\mbox{some member of $O(1)$]}  \enspace
\]

$O(1)$には別の問題もある。この記法には変数が入ってないので、どの変数が大きくなるのかわからないのだ。文脈から読み取る必要がある。上の例では、方程式の中に変数は$n$しかないので、$T(n)= 2 \log n + O(f(n))$の$f(n) = 1$であるものと読み取ることになる。

ビッグオー記法は新しいものでも、コンピュータサイエンス独自のものでもない。1894年には数学者Paul Bachmannが使っていた。その後しばらくしてコンピュータサイエンスにおいてアルゴリズムの実行時間を論ずるのに非常に便利なことが判明したのだ。
次のコードを考えてみましょう。

\javaimport{junk/Simple.snippet()}
\cppimport{ods/Simple.snippet()}

このメソッドを1回実行すると以下の処理が行われる。
\begin{itemize}
      \item 代入$1$回 (#int\, i\, =\, 0#)
      \item 比較$#n#+1$回 (#i < n#)
      \item インクリメント #n# 回(#i++#)
      \item 配列のオフセット計算 #n# 回 (#a[i]#)
      \item 関節代入#n#回 (#a[i] = i#)
\end{itemize}

よって実行時間は以下のようになる。
\[
    T(#n#)=a + b(#n#+1) + c#n# + d#n# + e#n# \enspace
\]

$a$、$b$、$c$、$d$、$e$はプログラムを実行するマシンに依存する定数で、それぞれ代入、比較、インクリメント、配列のオフセット計算、間接代入の実行時間を表す。しかしたった2行のコードの実行時間を表す式がこうだと、より複雑なコードやアルゴリズムをこのやり方では扱えないだろう。ビッグオー記法を使うと、実行時間は次のようにより単純になる。
\[
    T(#n#)= O(#n#) \enspace .
\]

この書き方はよりコンパクトながらさっきの式と同じくらいのことを教えてくれる。実行時間は定数$a$、$b$、$c$、$d$、$e$に依存している。これらの値がわからないと、実行時間はわからず比較できないのだ。これらの定数を明らかにするため努力しても（例えば実際に時間を測ってみる）、得られる結論はそのマシンにおいてのみ有効なだけだ。

ビッグオー記法を使うと、高い視点からより複雑な関数を分析することも可能だ。 2つのアルゴリズムのビッグオー記法での実行時間が同じなら、どちらが速いかわからず、はっきりとした勝ち負けがつかないかもしれない。あるマシンでは一方が速く、別のマシンでは他方が速いかもしれない。しかし2つのアルゴリズムのビッグオー記法での実行時間が異なることを示せれば、実行時間が小さい方は\emph{#n#が十分大ければ}速いとわかる。

ビッグオー記法を使って2つの異なる関数を比べる例を\figref{intro-asymptotics}示す。これは$f_1(#n#)=15#n#$と$f_2(n)=2#n#\log#n#$の増加を比べたものだ。$f_1(#n#)$は複雑な線形時間アルゴリズムの実行時間、$f_2(#n#)$は分割統治に基づくシンプルなアルゴリズムの実行時間だ。これを見ると、#n#が小さいうちは$f_1(#n#)$はより大きいが$f_2(#n#)$、#n#が大きくなると逆転することがわかる。そして、最終的には$f_1(#n#)$が圧倒的に性能がよくなるのだ。ビッグオー記法を使った解析で$O(#n#)\subset O(#n#\log #n#)$となることから、このことを知ることができる。

\begin{figure}
  \begin{center}
    \newlength{\tmpa}\setlength{\tmpa}{.98\linewidth}
    \addtolength{\tmpa}{-4mm}
    \resizebox{\tmpa}{!}{\input{images/bigoh-1.tex}}\\[4ex]
    \resizebox{.98\linewidth}{!}{\input{images/bigoh-2.tex}}
  \end{center}
  \caption{$15#n#$対$2#n#\log#n#$のプロット}
  \figlabel{intro-asymptotics}
\end{figure}

複数の変数を持つ関数に対して漸近表記を使用する場合もある。標準的な定義は定まっていないようだが、この本ためには次の定義で十分だ。

\[
   O(f(n_1,\ldots,n_k)) =
   \left\{\begin{array}{@{}l@{}}
             g(n_1,\ldots,n_k):\mbox{there exists $c>0$, and $z$ such that} \\
             \quad \mbox{$g(n_1,\ldots,n_k) \le c\cdot f(n_1,\ldots,n_k)$} \\
             \qquad \mbox{for all $n_1,\ldots,n_k$ such that $g(n_1,\ldots,n_k)\ge z$}
   \end{array}\right\} \enspace .
\]

この定義で我々が気にしていることがわかるのだ。引数$n_1,\ldots,n_k$が$g$を大きくするときのことだ。この定義は$f(n)$が$n$の増加関数なら一変数の場合の$O(f(n))$の定義と同じだ。我々の目的はこれでよいのだが、多変数の場合の漸近記法を異なる定義を与えている教科書もあることには注意が必要だ。

\subsection{ランダム性と確率}
\seclabel{randomization}

\index{randomization}%
\index{probability}%
\index{randomized data structure}%
\index{randomized algorithm}%
この本で扱うデータ構造には\emph{ランダム性}を利用するものがある。格納されているデータや実行された操作だけでなく、サイコロの目もふまえて動作を決めるのだ。そのため同じことをしても実行時間は毎回同じであるとは限らない。こういうデータ構造を分析するときは平均または\emph{期待実行時間}を考えるのがよい。
\index{expected running time}%
\index{running time!expected}%

形式的には、ランダム性を利用するデータ構造における操作の実行時間は確率変数である。そしせその\emph{期待値}を知りたい。全事象$U$の値をとる離散確率変数を$X$とするとき、$X$の期待値$E[X]$は以下のように定義される。
\index{expected value}%
\[
    \E[X] = \sum_{x\in U} x\cdot\Pr\{X=x\} \enspace
\]

ここで、$\Pr\{\mathcal{E}\}$は事象$\mathcal{E}$の発生確率とする。この本の例では、データ構造の内部で発生するランダム性のみを考慮して確率を定める。データ構造に入ってくるデータや実行される操作列がランダムだという仮定は置かないことに注意する。

期待値の最も重要な性質として\emph{期待値の線形性}がある。
\index{linearity of expectation}%
任意のふたつの確率変数$X$と$Y$について以下の関係が成り立つ。
\[
   \E[X+Y] = \E[X] + \E[Y] \enspace
\]

より一般的には、任意の確率変数$ X_1,\ldots,X_k $について以下の関係が成り立つ。
\[
   \E\left[\sum_{i=1}^k X_i\right] = \sum_{i=1}^k \E[X_i] \enspace
\]

期待値の線形性によって、（上の式の左辺のように）複雑な確率変数を(右辺のような)より単純な確率変数の和に分解できる。

\emph{インジケータ確率変数}\index{indicator random variable}はよく使う便利なトリックだ。この二値変数はなにかを数えたいときに役立つ。例を見るとよくわかるだろう。表裏が等しい確率で出るコインを$k$回投げたとき、表が出る回数の期待値を知りたいとする。
\index{coin toss}
直感的な答えは$k/2$だが、期待値の定義を使って証明しようとすると次のようになる。

\begin{align*}
   \E[X] & = \sum_{i=0}^k i\cdot\Pr\{X=i\} \\
         & = \sum_{i=0}^k i\cdot\binom{k}{i}/2^k \\
         & = k\cdot \sum_{i=0}^{k-1}\binom{k-1}{i}/2^k \\
         & = k/2 \enspace .
\end{align*}

この計算は$\Pr\{X=i\} = \binom{k}{i}/2^k$および2項係数の性質$i\binom{k}{i}=k\binom{k-1}{i}$や$\sum_{i=0}^{k} \binom{k}{i} = 2^{k}$を知っていないとできない。

インジケータ変数と期待値の線形性を使えばはるかに簡単になる。$\{1,\ldots,k\}$の各$i$に対し以下のインジケータの確率変数を定義する。

\[
    I_i = \begin{cases}
           1 & \text{$i$番目のコイントスの結果が表のとき} \\
           0 & \text{そうでないとき}
          \end{cases}
\]
そして、以下の計算を行う。
\[ \E[I_i] = (1/2)1 + (1/2)0 = 1/2 \enspace \]
ここで、$X=\sum_{i=1}^k I_i$なので以下のように所望の値を得られる。
\begin{align*}
   \E[X] & = \E\left[\sum_{i=1}^k I_i\right] \\
         & = \sum_{i=1}^k \E[I_i] \\
         & = \sum_{i=1}^k 1/2 \\
         & = k/2 \enspace .
\end{align*}

この計算は少し長いものの、不思議な等式や非自明な確率計算は必要ない。各コイントスは$1/2$の確率で表が出るので結果はたぶんコイン数の半分だ、という直感の説明でもある。

\section{計算モデル}
\seclabel{model}

本書ではデータ構造における操作の実行時間を理論的に分析する。これを正確に行うための計算の数学的なモデルが必要だ。そのために\emph{#w#ビットのワードRAM}モデルを使うことにする。
\index{word-RAM}%
\index{RAM}%
RAMはランダムアクセスマシン(Random Access Machine)の頭字語である。
このモデルではランダムアクセスメモリを使える。
ランダムアクセスメモリはセルの集まりで、これはそれぞれ#w#ビットのワードを格納できる。
\index{word}
つまり、各セルは$\{0,\ldots,2^{#w#}-1\}$の中のひとつを表せる。

ワードRAMモデルではワードの基本的な操作に一定の時間が必要である。基本的な操作は算術演算（#+#, #-#, #*#, #/#, #%#）や比較（$<$, $>$, $=$, $\le$, $\ge$）、ビット単位の論理演算（ビット単位のANDやOR、排他的論理和）である。

どのセルも一定の時間でに読み書きできる。コンピュータのメモリはメモリ管理システムによって管理される。メモリ管理システムは必要に応じてメモリブロックを割り当てたり割り当て解除したりしてくれる。サイズ$k$のメモリブロックの割当てには$O(k)$の時間がかかり、新しく割り当てられたメモリブロックへの参照（ポインタ）が返される。この参照はひとつのワードで表せる程度小さい。

ワード幅#w#はこのモデルの重要なパラメータである。
この本で#w#に置く仮定は、#n#をデータ構造に格納されうる要素数とするとき、$#w# \ge \log #n#$であるということだけだ。これは控えめな仮定である。なぜならこれが成り立たないとひとつのワードではデータ構造の要素数を数えることすらできないためである。

領域はワード単位で測るので、データ構造で使う領域の広さとはメモリの使用するワード数のことである。我々のデータ構造はみなジェネリック型Tの値を格納し、T型の要素は1ワードのメモリで表現できると仮定する。

この本に載っているデータ構造は、実装できないような特殊なトリックを使ってはいない。

% XXX: ここまで完了
\section{Correctness, Time Complexity, and Space Complexity}

When studying the performance of a data structure, there are three things that matter most:

\begin{description}
  \item[Correctness:] The data structure should correctly implement
    its interface.
    \index{correctness}
  \item[Time complexity:] The running times of operations on the data
    structure should be as small as possible.
    \index{time complexity}
    \index{complexity!time}
  \item[Space complexity:] The data structure should use as little memory
    as possible.
    \index{space complexity}
    \index{complexity!space}
\end{description}

%Sometimes these three requirements are in conflict with each other. For
%example, it may be possible to have a faster data structure by using
%more space.  It may be possible to have a data structure that is faster,
%or uses less space, if the data structure is allowed to make (hopefully
%occasional) mistakes.

In this introductory text, we will take correctness as a given;  we won't consider data structures that give incorrect answers to queries or don't perform updates properly.  We will, however, see data structures that make an extra effort to keep space usage to a minimum.  This won't usually affect the (asymptotic) running times of operations, but can make the data structures a little slower in practice.

When studying running times in the context of data structures we tend to come across three different kinds of running time guarantees:

\begin{description}
\item[Worst-case running times:] 
  \index{running time}
  \index{running time!worst-case}
  \index{worst-case running time}
  These are the strongest kind of running time guarantees.  If a data structure operation has a worst-case running time of $f(#n#)$, then one of these operations \emph{never} takes longer than $f(#n#)$ time.
\item[Amortized running times:]
  \index{running time!amortized}
  \index{amortized running time}
  If we say that the amortized running time of an operation in a data structure is $f(#n#)$, then this means that the cost of a typical operation is at most $f(#n#)$.  More precisely, if a data structure has an amortized running time of $f(#n#)$, then a sequence of $m$ operations takes at most $mf(#n#)$ time.  Some individual operations may take more than $f(#n#)$ time but the average, over the entire sequence of operations, is at most $f(#n#)$.
\item[Expected running times:] 
  \index{running time!expected}
  \index{expected running time}
  If we say that the expected running time of an operation on a data structure is $f(#n#)$, this means that the actual running time is a random variable (see \secref{randomization}) and the expected value of this random variable is at most $f(#n#)$.  The randomization here is with respect to random choices made by the data structure.
\end{description}

To understand the difference between worst-case, amortized, and expected running times, it helps to consider a financial example.  Consider the cost of buying a house: \paragraph{Worst-case versus amortized cost:} \index{amortized cost} Suppose that a home costs \$120\,000.  In order to buy this home, we might get a 120 month (10 year) mortgage with monthly payments of \$1\,200 per month.  In this case, the worst-case monthly cost of paying this mortgage is \$1\,200 per month.

If we have enough cash on hand, we might choose to buy the house outright, with one payment of \$120\,000.  In this case, over a period of 10 years, the amortized monthly cost of buying this house is \[ \$120\,000 / 120\text{ months} = \$1\,000\text{ per month} \enspace .  \] This is much less than the \$1\,200 per month we would have to pay if we took out a mortgage.

\paragraph{Worst-case versus expected cost:}
\index{expected cost} Next, consider the issue of fire insurance on our \$120\,000 home.  By studying hundreds of thousands of cases, insurance companies have determined that the expected amount of fire damage caused to a home like ours is \$10 per month.  This is a very small number, since most homes never have fires, a few homes may have some small fires that cause a bit of smoke damage, and a tiny number of homes burn right to their foundations.  Based on this information, the insurance company charges \$15 per month for fire insurance.

Now it's decision time. Should we pay the \$15 worst-case monthly cost for fire insurance, or should we gamble and self-insure at an expected cost of \$10 per month?  Clearly, the \$10 per month costs less \emph{in expectation}, but we have to be able to accept the possibility that the \emph{actual cost} may be much higher.  In the unlikely event that the entire house burns down, the actual cost will be \$120\,000.

These financial examples also offer insight into why we sometimes settle for an amortized or expected running time over a worst-case running time.  It is often possible to get a lower expected or amortized running time than a worst-case running time. At the very least, it is very often possible to get a much simpler data structure if one is willing to settle for amortized or expected running times.

\section{Code Samples}

\pcodeonly{

The code samples in this book are written in pseudocode.  \index{pseudocode} These should be easy enough to read for anyone who has any programming experience in any of the most common programming languages of the last 40 years.  To get an idea of what the pseudocode in this book looks like, here is a function that computes the average of an array, #a#: \pcodeimport{ods/Algorithms.average(a)} As this code illustrates, assigning to a variable is done using the $\gets$ notation.
% WARNING: graphic typesetting of assignment operator
We use the convention that the length of an array, #a#, is denoted by #len(a)# and array indices start at zero, so that #range(len(a))# are the valid indices for #a#.  To shorten code, and sometimes make it easier to read, our pseudocode allows for (sub)array assignments.
The following two functions are equivalent:
\pcodeimport{ods/Algorithms.left_shift_a(a).left_shift_b(a)}
The following code sets all the values in an array to zero:
\pcodeimport{ods/Algorithms.zero(a)}
When analyzing the running time of code like this, we have to be careful; statements like 
    #a[0:len(a)] = 1#
or
   #a[1:len(a)] = a[0:len(a)-1]#
do not run in constant time. They run in $O(#len(a)#)$ time.

We take similar shortcuts with variable assignments, so that the code #x,y=0,1# sets #x# to zero and #y# to 1 and the code #x,y = y,x# swaps the values of the variables #x# and #y#.  \index{swap}

Our pseudocode uses a few operators that may be unfamiliar.  As is standard in mathematics, (normal) division is denoted by the $/$ operator.  In many cases, we want to do integer division instead, in which case we use the $#//#$ operator, so that $#a//b# = \lfloor a/b\rfloor$ is the integer part of $a/b$.  So, for example, $3/2=1.5$ but $#3//2# = 1$.  \index{integer division} \index{div operator} Occasionally, we also use the $\bmod$ operator to obtain the remainder from integer division, but this will be defined when it is used.  \index{mod operator} \index{div operator} Later in the book, we may use some bitwise operators including left-shift (#<<#), right shift (#>>#), bitwise and (#&#), and bitwise exclusive-or (#^#).  \index{left shift} \index{#<<#|see {left shift}} \index{right shift} \index{#>>#|see {right shift}} \index{bitwise and} \index{#&#|see {bitwise and}} \index{#^#|see {bitwise exclusive-or}}

The pseudocode samples in this book are machine translations of Python code that can be downloaded from the book's website.\footnote{ \url{http://opendatastructures.org}}  If you ever encounter an ambiguity in the pseudocode that you can't resolve yourself, then you can always refer to the corresponding Python code.  If you don't read Python, the code is also available in Java and C++.  If you can't decipher the pseudocode, or read Python, C++, or Java, then you may not be ready for this book.  }

\notpcode{
The code samples in this book are written in the \lang\ programming language.  However, to make the book accessible to readers not familiar with all of \lang's constructs and keywords, the code samples have been simplified.  For example, a reader won't find any of the keywords #public#, #protected#, #private#, or #static#.  A reader also won't find much discussion about class hierarchies.  Which interfaces a particular class implements or which class it extends, if relevant to the discussion, should be clear from the accompanying text.

These conventions should make the code samples understandable by anyone with a background in any of the languages from the ALGOL tradition, including B, C, C++, C\#, Objective-C, D, Java, JavaScript, and so on.  Readers who want the full details of all implementations are encouraged to look at the \lang\ source code that accompanies this book.

This book mixes mathematical analyses of running times with \lang\ source code for the algorithms being analyzed.  This means that some equations contain variables also found in the source code.  These variables are typeset consistently, both within the source code and within equations.  The most common such variable is the variable #n# \index{n@#n#} that, without exception, always refers to the number of items currently stored in the data structure. 
}

\section{List of Data Structures}

Tables~\ref{tab:summary-i} and \ref{tab:summary-ii} summarize the performance of data structures in this book that implement each of the interfaces, #List#, #USet#, and #SSet#, described in \secref{interfaces}.  \Figref{dependencies} shows the dependencies between various chapters in this book.  \index{dependencies} A dashed arrow indicates only a weak dependency, in which only a small part of the chapter depends on a previous chapter or only the main results of the previous chapter.

\begin{table}
\vspace{56pt}
\begin{center}
\resizebox{.98\textwidth}{!}{
\begin{threeparttable}
\begin{tabular}{|l|l|l|l|} \hline
\multicolumn{4}{|c|}{#List# implementations} \\ \hline
 & #get(i)#/#set(i,x)# & #add(i,x)#/#remove(i)# & \\ \hline
#ArrayStack# & $O(1)$ & $O(1+#n#-#i#)$\tnote{A} & \sref{arraystack} \\
#ArrayDeque# & $O(1)$ & $O(1+\min\{#i#,#n#-#i#\})$\tnote{A} & \sref{arraydeque} \\
#DualArrayDeque# & $O(1)$ & $O(1+\min\{#i#,#n#-#i#\})$\tnote{A} & \sref{dualarraydeque}\\
#RootishArrayStack# & $O(1)$ & $O(1+#n#-#i#)$\tnote{A}  & \sref{rootisharraystack} \\
#DLList# & $O(1+\min\{#i#,#n#-#i#\})$ & $O(1+\min\{#i#,#n#-#i#\})$  & \sref{dllist} \\
#SEList# & $O(1+\min\{#i#,#n#-#i#\}/#b#)$ & $O(#b#+\min\{#i#,#n#-#i#\}/#b#)$\tnote{A}  & \sref{selist} \\
#SkiplistList# & $O(\log #n#)$\tnote{E} & $O(\log #n#)$\tnote{E}  & \sref{skiplistlist} \\ \hline
\multicolumn{4}{c}{} \\[2ex] \hline
\multicolumn{4}{|c|}{#USet# implementations} \\ \hline
 & #find(x)# & #add(x)#/#remove(x)# & \\ \hline
#ChainedHashTable# & $O(1)$\tnote{E} & $O(1)$\tnote{A,E} & \sref{hashtable} \\ 
#LinearHashTable# & $O(1)$\tnote{E} & $O(1)$\tnote{A,E} & \sref{linearhashtable} \\ \hline
\end{tabular}
\begin{tablenotes}
\item[A]{Denotes an \emph{amortized} running time.}
\item[E]{Denotes an \emph{expected} running time.}
\end{tablenotes}
\end{threeparttable}}
\end{center}
\caption[Summary of List and USet implementations.]{Summary of #List# and #USet# implementations.}
\tablabel{summary-i}
\end{table}

\begin{table}
\begin{center}
\begin{threeparttable}
\begin{tabular}{|l|l|l|l|} \hline
\multicolumn{4}{|c|}{#SSet# implementations} \\ \hline
 & #find(x)# & #add(x)#/#remove(x)# & \\ \hline
#SkiplistSSet# & $O(\log #n#)$\tnote{E} & $O(\log #n#)$\tnote{E} & \sref{skiplistset} \\ 
#Treap# & $O(\log #n#)$\tnote{E} & $O(\log #n#)$\tnote{E} & \sref{treap} \\ 
#ScapegoatTree# & $O(\log #n#)$ & $O(\log #n#)$\tnote{A} & \sref{scapegoattree} \\
#RedBlackTree# & $O(\log #n#)$ & $O(\log #n#)$ & \sref{redblacktree} \\ 
#BinaryTrie#\tnote{I} & $O(#w#)$ & $O(#w#)$ & \sref{binarytrie} \\ 
#XFastTrie#\tnote{I} & $O(\log #w#)$\tnote{A,E} & $O(#w#)$\tnote{A,E} & \sref{xfast} \\ 
#YFastTrie#\tnote{I} & $O(\log #w#)$\tnote{A,E} & $O(\log #w#)$\tnote{A,E} & \sref{yfast} \\ 
\javaonly{#BTree# & $O(\log #n#)$ & $O(B+\log #n#)$\tnote{A} & \sref{btree} \\ 
#BTree#\tnote{X} & $O(\log_B #n#)$ & $O(\log_B #n#)$ & \sref{btree} \\ } \hline
\multicolumn{4}{c}{} \\[2ex] \hline
\multicolumn{4}{|c|}{(Priority) #Queue# implementations} \\ \hline
 & #findMin()# & #add(x)#/#remove()# & \\ \hline
#BinaryHeap# & $O(1)$ & $O(\log #n#)$\tnote{A} & \sref{binaryheap} \\ 
#MeldableHeap# & $O(1)$ & $O(\log #n#)$\tnote{E} & \sref{meldableheap} \\ \hline
\end{tabular}
\begin{tablenotes}
\item[I]{This structure can only store #w#-bit integer data.}
\javaonly{\item[X]{This denotes the running time in the external-memory model; see \chapref{btree}.}}
\end{tablenotes}
%\renewcommand{\thefootnote}{\arabic{footnote}}
\end{threeparttable}
\end{center}
\caption[Summary of SSet and priority Queue implementations.]{Summary of #SSet# and priority #Queue# implementations.}
\tablabel{summary-ii}
\end{table}

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/dependencies}
  \end{center}
  \caption{The dependencies between chapters in this book.}
  \figlabel{dependencies}
\end{figure}

\section{Discussion and Exercises}

The #List#, #USet#, and #SSet# interfaces described in \secref{interfaces} are influenced by the Java Collections Framework \cite{oracle_collections}.  \index{Java Collections Framework} These are essentially simplified versions of the #List#, #Set#, #Map#, #SortedSet#, and #SortedMap# interfaces found in the Java Collections Framework.  \javaonly{The accompanying source code includes wrapper classes for making #USet# and #SSet# implementations into #Set#, #Map#, #SortedSet#, and #SortedMap# implementations.}

For a superb (and free) treatment of the mathematics discussed in this chapter, including asymptotic notation, logarithms, factorials, Stirling's approximation, basic probability, and lots more, see the textbook by Leyman, Leighton, and Meyer \cite{llm11}.  For a gentle calculus text that includes formal definitions of exponentials and logarithms, see the (freely available) classic text by Thompson \cite{t14}.

For more information on basic probability, especially as it relates to computer science, see the textbook by Ross \cite{r01}.  Another good reference, which covers both asymptotic notation and probability, is the textbook by Graham, Knuth, and Patashnik \cite{gkp94}.

\javaonly{Readers wanting to brush up on their Java programming can find many Java tutorials online \cite{oracle_tutorials}.}

\begin{exc}
  This exercise is designed to help familiarize the reader with choosing the right data structure for the right problem.  If implemented, the parts of this exercise should be done by making use of an implementation of the relevant interface (#Stack#, #Queue#, #Deque#, #USet#, or #SSet#) provided by the \javaonly{Java Collections Framework}\cpponly{C++ Standard Template Library}.

  Solve the following problems by reading a text file one line at a time and performing operations on each line in the appropriate data structure(s).  Your implementations should be fast enough that even files containing a million lines can be processed in a few seconds.
  \begin{enumerate}
    \item Read the input one line at a time and then write the lines out in reverse order, so that the last input line is printed first, then the second last input line, and so on.

    \item  Read the first 50 lines of input and then write them out in reverse order. Read the next 50 lines and then write them out in reverse order. Do this until there are no more lines left to read, at which point any remaining lines should be output in reverse order.

      In other words, your output will start with the 50th line, then the 49th, then the 48th, and so on down to the first line. This will be followed by the 100th line, followed by the 99th, and so on down to the 51st line. And so on.  Your code should never have to store more than 50 lines at any given time.

    \item Read the input one line at a time.  At any point after reading the first 42 lines, if some line is blank (i.e., a string of length 0), then output the line that occured 42 lines prior to that one. For example, if Line 242 is blank, then your program should output line 200. This program should be implemented so that it never stores more than 43 lines of the input at any given time.

    \item Read the input one line at a time and write each line to the output if it is not a duplicate of some previous input line. Take special care so that a file with a lot of duplicate lines does not use more memory than what is required for the number of unique lines.

    \item Read the input one line at a time and write each line to the output only if you have already read this line before. (The end result is that you remove the first occurrence of each line.) Take special care so that a file with a lot of duplicate lines does not use more memory than what is required for the number of unique lines.

    \item Read the entire input one line at a time. Then output all lines sorted by length, with the shortest lines first. In the case where two lines have the same length, resolve their order using the usual ``sorted order.''  Duplicate lines should be printed only once.

    \item Do the same as the previous question except that duplicate lines should be printed the same number of times that they appear in the input.

    \item Read the entire input one line at a time and then output the even numbered lines (starting with the first line, line 0) followed by the odd-numbered lines.

    \item Read the entire input one line at a time and randomly permute the lines before outputting them.  To be clear: You should not modify the contents of any line. Instead, the same collection of lines should be printed, but in a random order.
  \end{enumerate}
\end{exc}

\begin{exc}
  \index{Dyck word}
  A \emph{Dyck word} is a sequence of +1's and -1's with the property that the sum of any prefix of the sequence is never negative.  For example, $+1,-1,+1,-1$ is a Dyck word, but $+1,-1,-1,+1$ is not a Dyck word since the prefix $+1-1-1<0$.  Describe any relationship between Dyck words and #Stack# #push(x)# and #pop()# operations.
\end{exc}

\begin{exc}
  \index{matched string}
  \index{string!matched}
  A \emph{matched string} is a sequence of \{, \}, (, ), [, and ] characters that are properly matched.  For example, ``\{\{()[]\}\}'' is a matched string, but this ``\{\{()]\}'' is not, since the second \{ is matched with a ].  Show how to use a stack so that, given a string of length #n#, you can determine if it is a matched string in $O(#n#)$ time.
\end{exc}

\begin{exc}
  Suppose you have a #Stack#, #s#, that supports only the #push(x)# and #pop()# operations. Show how, using only a FIFO #Queue#, #q#, you can reverse the order of all elements in #s#.
\end{exc}

\begin{exc}
  \index{Bag@#Bag#}
  Using a #USet#, implement a #Bag#.  A #Bag# is like a #USet#---it supports the #add(x)#, #remove(x)# and #find(x)# methods---but it allows duplicate elements to be stored.  The #find(x)# operation in a #Bag# returns some element (if any) that is equal to #x#.  In addition, a #Bag# supports the #findAll(x)# operation that returns a list of all elements in the #Bag# that are equal to #x#.
\end{exc}

\begin{exc}
  From scratch, write and test implementations of the #List#, #USet# and #SSet# interfaces.  These do not have to be efficient.  They can be used later to test the correctness and performance of more efficient implementations.  (The easiest way to do this is to store the elements in an array.)
\end{exc}

\begin{exc}
  Work to improve the performance of your implementations from the previous question using any tricks you can think of.  Experiment and think about how you could improve the performance of #add(i,x)# and #remove(i)# in your #List# implementation.  Think about how you could improve the performance of the #find(x)# operation in your #USet# and #SSet# implementations.  This exercise is designed to give you a feel for  how difficult it can be to obtain efficient implementations of these interfaces.
\end{exc}




