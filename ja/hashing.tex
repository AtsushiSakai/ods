\chapter{ハッシュテーブル}
\chaplabel{hashtables}
\chaplabel{hashing}

ハッシュテーブルは大きな集合$U=\{0,\ldots,2^{#w#}-1\}$の要素#n#個（nは小さい整数）を格納するための効率的な方法だ。
\emph{ハッシュテーブル}という言葉が指すデータ構造はたくさんある。
\index{hash table}%
この章の前半ではハッシュテーブルの一般的な実装ふたつを紹介する。
これはチェイン、または線形探索を使うものだ。

ハッシュテーブルは整数でないデータを格納することもよくある。
この場合\emph{ハッシュ値}というデータに対応する値を使う。
\index{hash code}%
この章の後半ではハッシュ値の生成方法について説明する。

この章で扱う手法にはある範囲からランダムに生成した整数を利用する。
サンプルコードではこのランダム整数はハードコードされた定数になっている。
この定数は空気中のノイズを利用したランダムなビット列から得られる。

\section{#ChainedHashTable#: チェイン法を使ったハッシュテーブル}
\seclabel{hashtable}

\index{ChainedHashTable@#ChainedHashTable#}%
\index{chaining}%
\index{hashing with chaining}%
#ChainedHashTable#とは\emph{チェイン法}を使ってデータをリストの配列#t#に蓄えるデータ構造である。
整数#n#はすべてのリストにおける要素数の合計である。（\figref{chainedhashtable}を参照せよ。）
\codeimport{ods/ChainedHashTable.t.n}

\begin{figure}
   \begin{center}
     \includegraphics[width=\ScaleIfNeeded]{figs/chainedhashtable}
   \end{center}
   \caption[A ChainedHashTable]{An example of a #ChainedHashTable# with $#n#=14$ and $#t.length#=16$.  In this example $#hash(x)#=6$}
   \figlabel{chainedhashtable}
\end{figure}
\index{hash value}%
\index{hash(x)@#hash(x)#}%
データ#x#の\emph{ハッシュ値}#hash(x)#とは$\{0,\ldots,#t.length#-1\}$の中のある値である。
ハッシュ値が#i#であるデータはリスト#t[i]#に入れられる。
リストが長くなり過ぎないように、次の不変条件を保持する。
\[
    #n# \le #t.length#
\]
こうると、リストの平均要素数は常に1以下である。
$#n#/#t.length# \le 1$

ハッシュテーブルに要素#x#を追加するには配列#t#の大きさを増やす必要があるかどうかを確認し、必要があれば#t#を拡張する。
あとは#x#から$\{0,\ldots,#t.length#-1\}$内の整数であるハッシュ値#i#を計算し、#x#をリスト#t[i]#に追加すればよい。
\codeimport{ods/ChainedHashTable.add(x)}
配列を拡張するとき、#t#の大きさを二倍にし、元の配列に入っていた要素をすべて新しいテーブルに入れ直す。
これは#ArrayStack#のときと同じ戦略であり、あのときと同じ結果が適用できる。
すなわち、操作列についての拡張操作の償却実行時間は定数である。（必要ならページ\pageref{lem:arraystack-amortized}の\lemref{arraystack-amortized}を見直すこと。）

拡張のあとはは#x#を#ChainedHashTable#リスト#t[hash(x)]#に追加すればよい。
\ref{chap:arrays}章や\ref{chap:linkedlists}章で説明したどのリストの実装を使っても、この操作は定数時間で可能である。

要素#x#をハッシュテーブルから削除するためには、リスト#t[hash(x)]#を#x#が見つかるまで辿ればよい。
\codeimport{ods/ChainedHashTable.remove(x)}
この実行時間は$#n#_{#i#}$をリスト#t[i]#の長さとするとき、$O(#n#_{#hash(x)#})$である。

ハッシュテーブルから要素#x#を見つけるのも同様である。
リスト#t[hash(x)]#を線形に探索すればよい。
\codeimport{ods/ChainedHashTable.find(x)}
これもリスト#t[hash(x)]#の長さに比例する時間がかかる。

ハッシュテーブルの性能はハッシュ関数の選択に大きく影響される。
良いハッシュ関数は要素を#t.length#個のリストに均等に分散し、各リストの長さの期待値は$O(#n#/#t.length)# = O(1)$である。
一方、よくないハッシュ関数はすべての要素を同じリストに追加してしまう。
すなわち、リスト#t[hash(x)]#の長さは#n#になってしまう。
次の小節ではよいハッシュ関数について説明する。

\subsection{Multiplicative Hashing}
\seclabel{multihash}

\index{hashing!multiplicative}%
\index{multiplicative hashing}%
乗算ハッシュ法は剰余算術（\secref{arrayqueue}で説明した）と整数の割り算からハッシュ値をハッシュ値を計算する効率的な方法である。
$\ddiv$は割り算の商を求める演算子である。
形式的には任意の整数$a\ge 0$と$b\ge 1$について、$a\ddiv b = \lfloor a/b\rfloor$と定義される。

乗算ハッシュ法では、ある整数#d#（これは\emph{次数}と呼ばれるについて大きさ$2^{#d#}$であるハッシュテーブルを使う。
整数$#x#\in\{0,\ldots,2^{#w#}-1\}$のハッシュ値は次のように計算される。
\[
    #hash(x)# = ((#z#\cdot#x#) \bmod 2^{#w#}) \ddiv 2^{#w#-#d#}
\]
ここで#z#は$\{1,\ldots,2^{#w#}-1\}$のうちの奇数からランダムに選択される。
整数の演算は整数のビット数を$#w#$とするとき、$2^{#w#}$で勝手に剰余を取られることを利用すると、このハッシュ関数は非常に効率よく実現できる。
\footnote{ほとんどのプログラミング言語ではこうなのだが、残念なことにRuby（やPythonなど）ではそうではない。#w#ビットの固定桁の演算の結果がビットに収まらなくなったときには、可変桁数の整数表現が使われるのである。}
integer operation
that overflows is upgraded to a variable-length representation.}  
（\figref{multihashing}を参照せよ。）
さらに、$2^{#w#-#d#}$による整数の割り算は二進法で右側の$#w#-#d#$ビットを落とせば計算できる。（これはビットを右に$#w#-#d#$個だけシフトすればよく、実装は上の式よりも単純になる。）
\codeimport{ods/ChainedHashTable.hash(x)}

\begin{figure}
  \begin{center}
    \resizebox{.98\textwidth}{!}{
    \setlength{\arrayrulewidth}{1pt}
    \begin{tabular}{|lr@{}r|}\hline
    $2^#w#$ (4294967296)&            #1#&#00000000000000000000000000000000# \\
    #z# (4102541685)&                   &#11110100100001111101000101110101# \\
    #x# (42) &                          &#00000000000000000000000000101010# \\
    $#z#\cdot#x#$ &             #101000#&#00011110010010000101110100110010# \\
    $(#z#\cdot#x#)\bmod 2^{#w#}$ &      &#00011110010010000101110100110010# \\
    $((#z#\cdot#x#)\bmod 2^{#w#})\ddiv 2^{#w#-#d#}$ &&
                      \multicolumn{1}{@{}l|}{#00011110#} \\\hline
    \end{tabular}}
    \setlength{\arrayrulewidth}{.4pt}
  \end{center}
  \caption{$#w#=32$、$#d#=8$とした乗算ハッシュ法の操作}
  \figlabel{multihashing}
\end{figure}

次の補題は乗算ハッシュ法がうまくハッシュ値の衝突を避けることを示す。（証明はこの節の後半に回す。）

\begin{lem}\lemlabel{universal-hashing}
  #x#と#y#を$\{0,\ldots,2^{#w#}-1\}$内の任意の整数であって、$#x#\neq #y#$を満たすものとする。
  このとき$\Pr\{#hash(x)#=#hash(y)#\} \le 2/2^{#d#}$が成り立つ。
\end{lem}

\lemref{universal-hashing}より、#remove(x)#と#find(x)#の性能は簡単に解析できる。

\begin{lem}
任意のデータ#x#について、$#n#_{#x#}$を#x#がハッシュテーブルに現れる回数とするとき、リスト#t[hash(x)]#の長さの期待値は$#n#_{#x#} + 2$以下である。
\end{lem}

\begin{proof}
  $S$をハッシュテーブルに含まれる#x#ではない要素の集合とする。
  要素$#y#\in S$について、次の指示変数を定義する。
    \[ I_{#y#} = \left\{\begin{array}{ll}
       1 & \mbox{if $#hash(x)#=#hash(y)#$} \\
       0 & \mbox{otherwise}
       \end{array}\right.
    \]
  ここで、\lemref{universal-hashing}より、$\E[I_{#y#}] \le 2/2^{#d#}=2/#t.length#$である。
  リスト#t[hash(x)]#の長さの期待値は次のように求まる。
  \begin{eqnarray*}
   \E\left[#t[hash(x)].size()#\right] &=& \E\left[#n#_{#x#} + \sum_{#y#\in S} I_{#y#}\right] \\
    &=& #n#_{#x#} + \sum_{#y#\in S} \E [I_{#y#} ] \\
    &\le& #n#_{#x#} + \sum_{#y#\in S} 2/#t.length# \\
    &\le& #n#_{#x#} + \sum_{#y#\in S} 2/#n# \\
    &\le& #n#_{#x#} + (#n#-#n#_{#x#})2/#n# \\
    &\le& #n#_{#x#} + 2 \enspace ,
  \end{eqnarray*}
\end{proof}

Now, we want to prove 
続いて\lemref{universal-hashing}を証明する。
まずは整数論の定理からはじめる。
次の証明では$(b_r,\ldots,b_0)_2$と書いて、$\sum_{i=0}^r b_i2^i$を表す。ここで、$b_i$は0か1である。
すなわち、$(b_r,\ldots,b_0)_2$は二進表記で$b_r,\ldots,b_0$である整数のことである。
また、$\star$は値の不明な桁を表すとする。

\begin{lem}\lemlabel{hashing-mapping}
  $S$を$\{1,\ldots,2^{#w#}-1\}$内の奇数の集合とする。
  $q, i$は$S$の任意の要素とする。
  このとき、$#z#\in S$の要素が一意に存在して$#z#q\bmod 2^{#w#} = i$を満たす。
\end{lem}

\begin{proof}
  $#z#$を選ぶと$i$は決まるので、$#z#q\bmod 2^{#w#} = i$を満たす$#z#\in S$が一意に決まることを示せば良い。

  背理法で示す。
  整数#z#と#z'#が存在し$#z#>#z#'$であると仮定する。
  このとき、
  \[
     #z#q\bmod 2^{#w#} = #z#'q \bmod 2^{#w#} = i
  \]
  よって、
  \[
     (#z#-#z#')q\bmod 2^{#w#} = 0
  \]
  しかしこれはある整数$k$について次の式が成り立つことを意味する。
  \begin{equation}
    (#z#-#z#')q = k 2^{#w#} \eqlabel{factors}
  \end{equation}
  2進数のことを考えると
  \[
    (#z#-#z#')q = k\cdot(1,\underbrace{0,\ldots,0}_{#w#})_2
  \]
  なので、$(#z#-#z#')q$の末尾#w#桁はすべて0である。

  加えて、$q\neq 0$かつ$#z#-#z#'\neq 0$より$k\neq 0$である。
  $q$は奇数なのでこの二進表記の末尾桁は0ではない。
  \[
    q = (\star,\ldots,\star,1)_2
  \]
  $|#z#-#z#'| < 2^{#w#}$より、$#z#-#z#'$の末尾に連続して並ぶ0の個数は#w#未満である。
  \[
    #z#-#z#' = (\star,\ldots,\star,1,\underbrace{0,\ldots,0}_{<#w#})_2
  \]
  積$(#z#-#z#')q$の末尾に連続して並ぶ0の個数は#w#未満である。
  \[
   (#z#-#z#')q = (\star,\cdots,\star,1,\underbrace{0,\ldots,0}_{<#w#})_2 
    \enspace .
  \]
  以上より、$(#z#-#z#')q$は\myeqref{factors}を満たさず、矛盾する。
\end{proof}

\lemref{hashing-mapping}から次の便利な事実がわかる。
#z#が$S$から一様な確率でランダムに選ばれるとき、#zt#は$S$上に一様分布する。
次の証明では#z#の最下位の1である桁を除いた、$#w#-1$桁のランダムなビットを考えるのがポイントだ。

\begin{proof}[Proof of \lemref{universal-hashing}]
条件$#hash(x)#=#hash(y)#$と「$#z# #x#\bmod2^{#w#}$の上位#d#ビット$#z# #y#\bmod 2^{#w#}$の上位#d#ビットが等しい」は同値である。
この条件の必要条件は、$#z#(#x#-#y#)\bmod 2^{#w#}$の上位#d#ビットがすべて0である、またはすべて1であることである。
これは、$#zx#\bmod 2^{#w#} > #zy#\bmod 2^{#w#}$ならば次の条件である。
  \begin{equation}
      #z#(#x#-#y#)\bmod 2^{#w#} = 
      (\underbrace{0,\ldots,0}_{#d#},\underbrace{\star,\ldots,\star}_{#w#-#d#})_2 
      \eqlabel{all-zeros}
  \end{equation}
一方、$#zx#\bmod 2^{#w#} < #zy#\bmod 2^{#w#}$ならば次の条件である。
  \begin{equation}
      #z#(#x#-#y#)\bmod 2^{#w#} = 
      (\underbrace{1,\ldots,1}_{#d#},\underbrace{\star,\ldots,\star}_{#w#-#d#})_2 
       \enspace .
      \eqlabel{all-ones}
  \end{equation}
よって、$#z#(#x#-#y#)\bmod 2^{#w#}$が\myeqref{all-zeros}か\myeqref{all-ones}のどちらかであることを示せばよい。

  $q$を、ある整数$r\ge 0$が存在し、$(#x#-#y#)\bmod 2^{#w#}=q2^r$を満たす一意な奇数とする。
  \lemref{hashing-mapping}より、$#z#q\bmod 2^{#w#}$の二進表現は$#w#-1$桁のランダムなビットを持つ。（最下位桁は1である。）
  \[
   #z#q\bmod 2^{#w#}  = (\underbrace{b_{#w#-1},\ldots,b_{1}}_{#w#-1},1)_2
  \]
  よって$#z#(#x#-#y#)\bmod 2^{#w#}=#z#q2^r\bmod 2^{#w#}$は桁の$#w#-r-1$ランダムなビットを持つ。（その後1が続き、さらに$r$個の0が続く。）
  \[
  #z#(#x#-#y#)\bmod 2^{#w#}  =
  #z#q2^{r}\bmod 2^{#w#} =
      (\underbrace{b_{#w#-r-1},\ldots,b_{1}}_{#w#-r-1},1,\underbrace{0,0,\ldots,0}_{r})_2
  \]
  これで証明が終わる。
  $r > #w#-#d#$ならば$#z#(#x#-#y#)\bmod 2^{#w#}$の上位#d#ビットは0と1を共に含む。よって$#z#(#x#-#y#)\bmod 2^{#w#}$が\myeqref{all-zeros}または\myeqref{all-ones}である確率は0である。
  $#r#=#w#-#d#$ならば\myeqref{all-zeros}の確率は0だが、\myeqref{all-ones}である確率は$1/2^{#d#-1}=2/2^{#d#}$である。
  （これは$b_1,\ldots,b_{d-1}=1,\ldots,1$である必要があるためだ。）
  $r < #w#-#d#$ならば$b_{#w#-r-1},\ldots,b_{#w#-r-#d#}=0,\ldots,0$、すなわち$b_{#w#-r-1},\ldots,b_{#w#-r-#d#}=1,\ldots,1$である。
  いずれの場合の確率も$1/2^{#d#}$であり、またそれぞれの事象は互いに排反である。
  よって、このどちらかである確率は$2/2^{#d#}$である。
\end{proof}

\subsection{要約}

次の定理は#ChainedHashTable#の性能をまとめたものだ。

\begin{thm}\thmlabel{hashtable}
  #ChainedHashTable#は#USet#インターフェースを実装する。
  #grow()#のコストを無視すると、#ChainedHashTable#における#add(x)#・#remove(x)#・#find(x)#の期待実行時間は$O(1)$である。

  さらに、空の#ChainedHashTable#に対して、$m$個の#add(x)#・#remove(x)#からなる任意の操作列を順に実行するとき、#grow()#の呼び出しに要する合計時間は$O(m)$である。
\end{thm}

\section{#LinearHashTable#：線形探索法}
\seclabel{linearhashtable}

\index{LinearHashTable@#LinearHashTable#}%
The 
#ChainedHashTable#はリストの配列を使うデータ構造であった。
#i#番目のリストは$#hash(x)#=#i#$である#x#を全て格納していた。
\emph{オープンアドレス法}と呼ばれる別の方法があり、これは配列#t#に直列要素を収めるものだ。
\index{open addressing}%
このやり方はこの節で説明する#LinearHashTable#が採用しているものだ。
文献によっては\emph{線形探索法によるオープンアドレス}と呼ばれることもある。
\index{linear probing}%

#LinearHashTable#の背後にあるアイデアは#i=hash(x)#である要素#x#を理想的には#t[i]#に入れたい、というものだ。
もしこれが（他の要素が既にそこに入っていて）ムリなら、$#t#[(#i#+1)\bmod#t.length#]$に要素を入れてみる。
これもムリなら$#t#[(#i#+2)\bmod#t.length#]$に入れてみる。
これを#x#が入れる場所が見つかるまで繰り返す。

#t#の値は次の三種類のいずれかだ。
\begin{enumerate}
  \item データの値：#USet#に入っている実際の値である。
  \item #null#：データが入っていないことを示す。
  \item #del#：データが入っていたがそれが削除されたことを示す。
\end{enumerate}
#LinearHashTable#の要素数を数えるカウンタ#n#に加えて、上の一つ目と三つ目の個数の合計を数えるカウンタ#q#を用意する。
#q#の値は#n#に#del#の個数を加えた値である。
効率的にこれを実装するために、#t#は#q#より十分大きい必要がある。このとき、#t#には#null#である場所がたくさんある。
よって#LinearHashTable#の操作は不変条件$#t.length#\ge 2#q#$を常に満たすようにする。

整理すると、#LinearHashTable#は要素の配列#t#に加え、整数#n#、#q#を持つ。これはそれぞれ要素数と、#null#でない値の個数を保持する。
さらに、ハッシュ関数の値域の大きさ2の冪に制限されていることが多いので、整数不変条件$#t.length#=2^#d#$を満たす整数#d#も持ち回る。
\codeimport{ods/LinearHashTable.t.n.q.d}

#LinearHashTable#の#find(x)#操作は単純である。
$#i#=#hash(x)#$として、#t[i]#, $#t#[(#i#+1)\bmod #t.length#]$, $#t#[(#i#+2)\bmod #t.length#]$, ...と順に、#t[i']=x#または#t[i']=null#を満たす添え字#i'#を探す。
#t[i']=x#のとき、#x#が見つかったとして#t[i']#を返す。
#t[i']=null#のとき、#x#はハッシュテーブルに含まれないとして#null#を返す。
\codeimport{ods/LinearHashTable.find(x)}

#LinearHashTable#の#add(x)#操作も簡単に実装できる。
#find(x)#を使えば#x#が入っているかどうか確認できる。
#x#が入っていなければ#t[i]#, $#t#[(#i#+1)\bmod #t.length#]$, $#t#[(#i#+2)\bmod #t.length#]$, ...と順に探し、#null#か#del#を見つけたらそこを#x#に書き換え、#n#と#q#をひとつずつ増やす。
\codeimport{ods/LinearHashTable.add(x)}

ここまでで#remove(x)#の実装も明らかだろう。
t[i]#, $#t#[(#i#+1)\bmod #t.length#]$, $#t#[(#i#+2)\bmod #t.length#]$, ...と#t[i']=x#または#t[i']=null#である添え字#i'#を見つけるまで探す。
#t[i']=x#ならば#t[i']=del#とし#true#を返す。
#t[i']=null#ならば#x#はテーブルに入っていなかった（そのため削除できない）として#false#を返す。
\codeimport{ods/LinearHashTable.remove(x)}

#find(x)#・#add(x)#・#remove(x)#の正しさは簡単に確認できる。
ただし、これは#del#を使うことに依存している。
これらの操作は#null#でない値を#null#に書き換えないことに注意する。
そのため#t[i']=null#である添え字#i'#を見つけると、#x#は配列に入っていないことがわかる。
#t[i']#はずっと#null#であったといえるので先立って、すなわち#i'#よりも先の添字に#add(x)#が要素を追加していることはないのである。


#null#でないエントリの数が$#t.length#/2$より大きいときに#add(x)#を呼ぶとき、またはデータの入っているエントリ数が#t.length/8#よりも小さいときに#remove(x)#を呼ぶと#resize()#が呼ばれる。
#resize()#は他の配列を使ったデータ構造の場合と同様に働く。
まず$2^{#d#} \ge 3#n#$を満たす最小の非負整数#d#を見つける。
大きさ$2^{#d#}$の配列#t#を割当て、古い配列の要素を全て移し替える。
この処理の過程で、#q#を#n#に等しくリセットする。
これは新しい配列#t#は#del#を含まないためである。
\codeimport{ods/LinearHashTable.resize()}

\subsection{線形探索法の解析}

#add(x)#・#remove(x)#・#find(x)#のいずれも#null#であるエントリを見つけると（あるいはその前に）終了することに注意する。
配列#t#の半分以上は#null#なので、直感的には線形探索法はすぐに#null#のエントリを見つけて処理を終えそうに思う。
しかしあまりこの直感を当てにはできない。
例えば#t#のエントリを平均的には２つだけ見れば良さそうだが、実はこれは正しくない。

この節ではハッシュ値は$\{0,\ldots,#t.length#-1\}$から一様な確率分布に従う独立な値であると仮定する。
これは現実的な仮定ではないが、これを仮定すれば線形探索法の解析が可能になる。
この節の後半でTabulation Hashingという、線形探索法の用途には「十分よい」ハッシュ法を説明する。
もうひとつ、#t#の添字はすべて#t.length#で剰余を取っているとする。
つまり単に#t[i]#と書いても$#t#[#i#\bmod#t.length#]$のことである。

\index{run}%
XXX: runの訳語
\emph{#i#から始まる長さ$k$のrun}が発生するとはテーブルのエントリ$#t[i]#, #t[i+1]#,\ldots,#t#[#i#+k-1]$がいずれも#null#でなく、$#t#[#i#-1]=#t#[#i#+k]=#null#$であることをいう。
#t#の#null#でない要素の数は#q#で、#add(x)#は常に$#q#\le#t.length#/2$であることを保証する。
直前の#rebuild()#以後、#t#に挿入された#q#個の要素を$#x#_1,\ldots,#x#_{#q#}$とする。
仮定より、ハッシュ値$#hash#(#x#_j)$はいずれも一様分布に従う互いに独立な確率変数である。
ここまでの準備で線形探索法の解析における主要な補題を示せる。

\begin{lem}\lemlabel{linear-probing}
#i#を$\{0,\ldots,#t.length#-1\}$のある要素に固定する。
このときある定数$c(0<c<1)$が存在して、#i#から始まる長さ$k$のrunが発生する確率を$O(c^k)$と表せる。
\end{lem}

\begin{proof}
#i#から始まる長さ$k$のrunが発生するとき、相異なる$k$個の要素$#x#_j$が存在し、$#hash#(#x#_j)\in\{#i#,\ldots,#i#+k-1\}$を満たす。
この事象の発生確率は次のように計算できる。
\[
  p_k  = \binom{#q#}{k}\left(\frac{k}{#t.length#}\right)^k\left(\frac{#t.length#-k}{#t.length#}\right)^{#q#-k}
\]
これは$k$個の要素の選び方によらず、これら$k$個の要素はいずれも$k$箇所のうちのいずれかに、そして残りの$#q#-k$個の要素は残りの$#t.length#-k$箇所に割り振られなければならないためだ。
\footnote{$p_k$は#i#から始まる長さ$k$のrunが発生する確率よりも大きいことに注意する。これは$p_k$は必要条件$#t#[#i#-1]=#t#[#i#+k]=#null#$を要求しないためである。}

次の導出ではすこしズルをしている。
$r!$を$(r/e)^r$に置き換える部分である。
スターリング近似（\secref{factorials}）からこれは真実からのずれは$O(\sqrt{r})$程度だとわかる。
これを許すと導出が簡単になるのである。
\excref{linear-probing}ではスターリング近似を使ったより厳密な計算を読者にやってもらう予定だ。

#t.length#が最小値を取るとき$p_k$は最大値を取る。
またデータ構造は不変条件$#t.length# \ge 2#q#$を保つ。
よって次の式が成り立つ。
\begin{align*}
   p_k & \le \binom{#q#}{k}\left(\frac{k}{2#q#}\right)^k\left(\frac{2#q#-k}{2#q#}\right)^{#q#-k} \\
  & = \left(\frac{#q#!}{(#q#-k)!k!}\right)\left(\frac{k}{2#q#}\right)^k\left(\frac{2#q#-k}{2#q#}\right)^{#q#-k} \\
  & \approx \left(\frac{#q#^{#q#}}{(#q#-k)^{#q#-k}k^k}\right)\left(\frac{k}{2#q#}\right)^k\left(\frac{2#q#-k}{2#q#}\right)^{#q#-k} && \text{[Stirling's approximation]} \\
  & = \left(\frac{#q#^{k}#q#^{#q#-k}}{(#q#-k)^{#q#-k}k^k}\right)\left(\frac{k}{2#q#}\right)^k\left(\frac{2#q#-k}{2#q#}\right)^{#q#-k} \\
 & = \left(\frac{#q#k}{2#q#k}\right)^k
     \left(\frac{#q#(2#q#-k)}{2#q#(#q#-k)}\right)^{#q#-k} \\
 & = \left(\frac{1}{2}\right)^k
     \left(\frac{(2#q#-k)}{2(#q#-k)}\right)^{#q#-k} \\
 & = \left(\frac{1}{2}\right)^k
     \left(1+\frac{k}{2(#q#-k)}\right)^{#q#-k} \\
 & \le \left(\frac{\sqrt{e}}{2}\right)^k
\end{align*}
最後の変形では$x>0$ならば$(1+1/x)^x \le e$であることを利用した。
ここで、$\sqrt{e}/{2}< 0.824360636 < 1$なので、補題が示された。
\end{proof}

\lemref{linear-probing}を使えば#find(x)#・#add(x)#・#remove(x)#の期待実行時間の上界は直接的に計算できる。
まずは最もシンプルな#find(x)#を呼ぶが#x#が#LinearHashTable#に入っていないときを考える。
この場合$#i#=#hash(x)#$は$\{0,\ldots,#t.length#-1\}$の値を取り、#t#の中身と独立な確率変数である。
#i#が長さ$k$のrunの一部なら、#find(x)#の実行時間は$O(1+k)$以下である。
よって実行時間の期待値の上界を計算できる。
\[
  O\left(1 + \left(\frac{1}{#t.length#}\right)\sum_{i=1}^{#t.length#}\sum_{k=0}^{\infty} k\Pr\{\text{#i# is part of a run of length $k$}\}\right)
\]
内側の和を取っている長さ$k$のrunはk回カウントされているので、これをまとめて$k^2$とすれば、上の和は次のように変形できる。
\begin{align*}
  & { } O\left(1 + \left(\frac{1}{#t.length#}\right)\sum_{i=1}^{#t.length#}\sum_{k=0}^{\infty} k^2\Pr\{\mbox{#i# starts a run of length $k$}\}\right) \\
  & \le O\left(1 + \left(\frac{1}{#t.length#}\right)\sum_{i=1}^{#t.length#}\sum_{k=0}^{\infty} k^2p_k\right) \\
  & = O\left(1 + \sum_{k=0}^{\infty} k^2p_k\right) \\
  & = O\left(1 + \sum_{k=0}^{\infty} k^2\cdot O(c^k)\right) \\
  & = O(1)
\end{align*}
最後の変形$\sum_{k=0}^{\infty} k^2\cdot O(c^k)$では指数級数の性質を使っている。
\footnote{解析学の教科書ではこの和は比を計算して求める。すなわち、ある正の数$k_0$が存在し、任意の$k\ge k_0$について、$\frac{(k+1)^2c^{k+1}}{k^2c^k} < 1$を満たす。}
以上より、#LinearHashTable#に入っていない#x#について、#find(x)#の期待実行時間は$O(1)$である。

#resize()#のコストを無視していいなら、この解析で#LinearHashTable#の解析を終わりだ。

まず上の#find(x)#の解析は、#add(x)#において#x#がテーブルに含まれないときにもそのまま適用できる。
#x#がテーブルに含まれるときの#find(x)#の解析は#add(x)#によって#x#を加えたときのコストと同じである。
最後に、#remove(x)#のコストも#find(x)#のコストと同じだ。

まとめると、#resize()#のコストを無視すれば、#LinearHashTable#の操作の期待実行時間はいずれも$O(1)$である。
リサイズのコストを考える場合は、\secref{arraystack}で#ArrayStack#の償却解析を行ったのと同様である。

\subsection{Summary}

次の定理は#LinearHashTable#の性能をまとめたものだ。

\begin{thm}\thmlabel{linear-probing}
  #LinearHashTable#は#USet#インターフェースを実装する。
  #resize()#のコストを無視すると、#LinearHashTable#における#add(x)#・#remove(x)#・#find(x)#の期待実行時間は$O(1)$である。

  さらに、空の#LinearHashTable#に対して、$m$個の#add(x)#・#remove(x)#からなる操作の列を順に実行するとき、#resize()#にかかる時間の合計は$O(m)$である。
\end{thm}

\subsection{Tabulation Hashing}
\seclabel{tabulation}

\index{tabulation hashing}%
#LinearHashTable#の解析では強い仮定を置いていた。
すなわち、任意の相異なる要素$\{#x#_1,\ldots,#x#_#n#\}$についてそのハッシュ値$#hash#($x$_1),\ldots,#hash#(#x#_#n#)$が独立に一様な確率で$\{0,\ldots,#t.length#-1\}$内を分布するという仮定である。
これを実現するひとつのやり方は大きさ$2^{#w#}$の巨大な配列#tab#を準備し、すべてのエントリを互いに独立な#w#-bitの乱数で初期化することだ。
このとき、#hash(x)#は#tab[x.hashCode()]#から#d#ビットを整数として取り出せばよい。
\codeimport{ods/LinearHashTable.idealHash(x)}
あいにく大きさ$2^{#w#}$の配列はメモリ使用量の観点から現実的でない。
\emph{Tabulation Hashing}では#w#ビットの整数の代わりに$#w#/#r#$個の$#r#$ビット整数で妥協する。
こうすれば大きさ$2^{#r#}$の配列$#w#/#r#$個で済むのである。
これらの配列に入っている整数はいずれも互いに独立な#w#ビットの乱数である。
-bit integers.  To obtain the value
of 
#hash(x)#を計算するために、#x.hashCode()#を$#w#/#r#$個の#r#ビット整数に分け、それぞれを配列の添え字として使う。
その後各配列の値をビット単位の排他的論理和を計算し、この結果を#hash(x)#とする。
次のコードは$#w#=32, #r#=4$の場合のものである。
\codeimport{ods/LinearHashTable.hash(x)}
この場合、#tab#は4つの列と$2^{32/4}=256$の行からなる二次元配列である。
XXX: これ必要?
\pcodeonly{
Quantities like #0xff#, used above, are \emph{hexadecimal numbers}
\index{hexadecimal numbers}%
whose digits have 16 possible values 0--9, which have their usual
meaning and a--f, which denote 10--15.  The number $#0xff#=15\cdot 16 + 15 = 255$.
The #&# symbol is the \emph{bitwise and} operator, so
code like #h >> 8 & 0xff# extracts bits with index 8 through 15 of #h#.}

任意の#x#について#hash(x)#は$\{0,\ldots,2^{#d#}-1\}$の値をを一様な確率で取ることを簡単に確認できる。
少し計算すればハッシュ値のペアが互いに独立であることも確認できる。
これは#ChainedHashTable#における乗算ハッシュ法の代わりにTabulation Hashingを使えることを意味する。

残念ながら、相異なる任意の#n#個の値の組みについて、そのハッシュ値が互いに独立というわけではない。
しかしそうであっても、Tabulation Hashingは\thmref{linear-probing}で示した性質を保証するのに十分よいハッシュ法である。
この話題についてはこの章の終わりで参考文献を紹介する。
